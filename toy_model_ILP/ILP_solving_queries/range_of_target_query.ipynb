{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29ba360-5460-4173-b3c2-461e46b15002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import io\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from itertools import combinations\n",
    "from itertools import groupby\n",
    "from random import sample\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": ['Computer Modern Roman'],\n",
    "})\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3f2d06-15c8-4e00-9dcb-b3aa4f6e3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_sols(n,m,quer_dict,noise,n_sols,partial_dict = dict()):\n",
    "    \"\"\" builds ILP to construct candidate database, based on answers to queries \n",
    "    Assumes bound on noise, n and m is known. \"\"\"\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    # Initialize the decision variables\n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "                   for j in range(m)] for i in range(n)])\n",
    "    # print(x)\n",
    "    # partial_dict = dict()\n",
    "    #adding the constraints...\n",
    "    quer_cons(M,x,n,quer_dict,noise)\n",
    "    partial_cons(M,x,n,partial_dict)\n",
    "            \n",
    "    # for i in range(n-1):\n",
    "    #     M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    bin_cons(M,x,n)\n",
    "\n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = n_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    return M.SolCount\n",
    "\n",
    "def gen_sols(n,m,quer_dict,partial_dict,noise,n_sols):\n",
    "    \"\"\" builds ILP to construct candidate database, based on answers to queries \n",
    "    Assumes bound on noise, n and m is known. \"\"\"\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    # Initialize the decision variables\n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "                   for j in range(m)] for i in range(n)])\n",
    "    # print(x)\n",
    "    \n",
    "    #adding the constraints...\n",
    "    quer_cons(M,x,n,quer_dict,noise)\n",
    "    partial_cons(M,x,n,partial_dict)\n",
    "            \n",
    "    # for i in range(n-1):\n",
    "    #     M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    bin_cons(M,x,n)\n",
    "\n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = n_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    \n",
    "    # print(f\"Took {M.Runtime:.2f} seconds to solve\")\n",
    "    \n",
    "    if M.solCount == 0:\n",
    "        print(\"infeasible\")\n",
    "        \n",
    "    print(f'old_ILP: {M.SolCount} solutions')\n",
    "    \n",
    "    out_lst = []\n",
    "    for k in range(M.SolCount):\n",
    "        M.Params.SolutionNumber = k\n",
    "        out_x = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[0])):\n",
    "                out_x[i][j] = x[i][j].Xn\n",
    "        # print([var.Xn for var in m.getVars()])\n",
    "        out_lst.append(out_x)\n",
    "        \n",
    "    return out_lst, M.Runtime\n",
    "\n",
    "def quer_cons(M,x,n,quer_dict,noise):\n",
    "    \"\"\" Adds the constraints based on the queries in quer_dict\"\"\"\n",
    "    \n",
    "    y_var_dict = defaultdict()\n",
    "    y_p_var_dict = defaultdict()\n",
    "    y_n_var_dict = defaultdict()\n",
    "    \n",
    "    for quer in quer_dict.keys():\n",
    "        y_var_dict[quer] = []\n",
    "        y_p_var_dict[quer] = []\n",
    "        y_n_var_dict[quer] = []\n",
    "        \n",
    "        positions_0 = []\n",
    "        positions_1 = []\n",
    "        for i, j in zip(quer[0],quer[1]):\n",
    "            if j == 0: \n",
    "                positions_0.append(i)\n",
    "            elif j ==1:\n",
    "                positions_1.append(i)\n",
    "        for i in range(n):\n",
    "            y_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_{quer}_{i}\"))\n",
    "            y_p_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_p_{quer}_{i}\"))\n",
    "            y_n_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_n_{quer}_{i}\"))\n",
    "            \n",
    "            M.addConstr( len(quer[0])*y_var_dict[quer][i] + (len(quer[0])+0.5)*y_p_var_dict[quer][i] <= \n",
    "                        sum([x[i][j] for j in positions_1]) + sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( (len(quer[0])-0.5)*y_n_var_dict[quer][i] + len(quer[0])*y_var_dict[quer][i] + \n",
    "                        m*y_p_var_dict[quer][i] >= \n",
    "                        sum([x[i][j] for j in positions_1])+ sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( y_var_dict[quer][i] + y_p_var_dict[quer][i] + y_n_var_dict[quer][i] == 1)\n",
    "            \n",
    "        if noise == 0:\n",
    "            M.addConstr(sum(y_var_dict[quer]) == quer_dict[quer])\n",
    "        else:\n",
    "            M.addConstr(sum(y_var_dict[quer]) <= quer_dict[quer] + noise)\n",
    "            M.addConstr(sum(y_var_dict[quer]) >= quer_dict[quer] - noise)\n",
    "    return None\n",
    "\n",
    "def partial_cons(M,x,n,partial_dict):\n",
    "    \"\"\" Adds the constraints based on the partial information in partial_dict\"\"\"\n",
    "    \n",
    "    y_var_dict_partial = defaultdict()\n",
    "    y_p_var_dict_partial = defaultdict()\n",
    "    y_n_var_dict_partial = defaultdict()\n",
    "    \n",
    "    for quer in partial_dict.keys():\n",
    "        y_var_dict_partial[quer] = []\n",
    "        y_p_var_dict_partial[quer] = []\n",
    "        y_n_var_dict_partial[quer] = []\n",
    "        \n",
    "        positions_0 = []\n",
    "        positions_1 = []\n",
    "        for i, j in zip(quer[0],quer[1]):\n",
    "            if j == 0: \n",
    "                positions_0.append(i)\n",
    "            elif j ==1:\n",
    "                positions_1.append(i)\n",
    "        for i in range(n):\n",
    "            y_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_partial_{quer}_{i}\"))\n",
    "            y_p_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_p_partial_{quer}_{i}\"))\n",
    "            y_n_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_n_partial_{quer}_{i}\"))\n",
    "            \n",
    "            M.addConstr( len(quer[0])*y_var_dict_partial[quer][i] + (len(quer[0])+0.5)*y_p_var_dict_partial[quer][i] <= \n",
    "                        sum([x[i][j] for j in positions_1]) + sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( (len(quer[0])-0.5)*y_n_var_dict_partial[quer][i] + len(quer[0])*y_var_dict_partial[quer][i] + \n",
    "                        m*y_p_var_dict_partial[quer][i] >= \n",
    "                        sum([x[i][j] for j in positions_1])+ sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( y_var_dict_partial[quer][i] + y_p_var_dict_partial[quer][i] + y_n_var_dict_partial[quer][i] == 1)\n",
    "            \n",
    "        # this line does the work\n",
    "        M.addConstr(sum(y_var_dict_partial[quer]) >= partial_dict[quer])\n",
    "        \n",
    "    return None\n",
    "\n",
    "def bin_cons(M,x,n):\n",
    "    \"\"\" Adds binary symmetry breaking constraint \"\"\"\n",
    "    for i in range(n-1):\n",
    "        M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    return None\n",
    "\n",
    "def gen_bin_data_set(n,m):\n",
    "    \"\"\"n is number of people, m is number of attributes used, database is uniform random\"\"\"\n",
    "    db = pd.DataFrame(np.random.randint(0,2,size=(n, m)), columns=[f'att_{x}' for x in range(m)])\n",
    "    return db\n",
    "\n",
    "def gen_powerset(k):\n",
    "    \"\"\" generates a powerset of k elements \"\"\"\n",
    "    out = []\n",
    "    for i in itertools.product([0,1],repeat=k):\n",
    "        out.append(i)\n",
    "    return out\n",
    "\n",
    "def gen_queries_uniform_complete(m,n_queries):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return sample(queries,min(n_queries,len(queries)))\n",
    "\n",
    "def gen_queries_all(m):\n",
    "    \"\"\"Generates all possible queries of m binary attributes\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return queries\n",
    "\n",
    "def gen_queries_comp(m):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return queries\n",
    "\n",
    "def gen_queries_uniform(m,n_queries):\n",
    "    \"\"\"Generates random queries. Chooses number of attributes uniformly over [1,m],\n",
    "    Would be interested to see how this changes with a skewed distribution, also\n",
    "    would be interesting how it changes if symmetry in queries is forced\"\"\"\n",
    "    \n",
    "    queries = []\n",
    "\n",
    "    for i in range(n_queries):\n",
    "        quer = []\n",
    "        atts = np.random.randint(1,m+1)\n",
    "        quer.append(tuple(sorted(sample(range(m),atts))))\n",
    "        quer.append(tuple(np.random.choice([0,1], size=[atts])))\n",
    "        queries.append(quer)\n",
    "\n",
    "    queries = list(set([tuple(x) for x in queries]))\n",
    "\n",
    "    return queries\n",
    "\n",
    "def gen_partial_info(db,n_part,m_part):\n",
    "    \"\"\" generates a partial info dict based on n_partials rows\n",
    "    and m_partials attributes per individual, if no partial info, we get None type \"\"\"\n",
    "    \n",
    "    n = db.shape[0]\n",
    "    m = db.shape[1]\n",
    "    \n",
    "    partial_dict = defaultdict(lambda:0)\n",
    "    \n",
    "    if n_part == 0 or m_part == 0:\n",
    "        return partial_dict\n",
    "    \n",
    "    row_inds = np.random.choice(n,n_part, replace = False)\n",
    "    for i in row_inds:\n",
    "        att_sample = np.random.choice(m,m_part, replace = False)\n",
    "        lst = []\n",
    "        for j in att_sample:\n",
    "            lst.append(int(db[i:i+1][f\"att_{j}\"]))\n",
    "            \n",
    "        # print(att_sample,lst)\n",
    "        vals = [x for _,x in sorted(zip(att_sample,lst))]\n",
    "        \n",
    "        partial_dict[(tuple(sorted(att_sample)),tuple(vals))] += 1\n",
    "    return partial_dict\n",
    "            \n",
    "def quer2string(query):\n",
    "    \"\"\" converts query to a string to be used by pandas 'query' function\"\"\"\n",
    "    string = \"\"\n",
    "    for pos, att in enumerate(query[0]):\n",
    "        string += f'att_{att} == {query[1][pos]} & '\n",
    "    return string[0:-3]\n",
    "\n",
    "def get_counts_uniform(db,quers,noise):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + np.random.randint(-noise,noise+1),0)\n",
    "    return out_dict\n",
    "\n",
    "def binatodeci(binary):\n",
    "    return sum(val*(2**idx) for idx, val in enumerate(reversed(binary)))\n",
    "\n",
    "def check_fixed_sols(sols):\n",
    "    \"\"\"Takes some candidate databases and returns which\n",
    "    rows are contained in all of them, including duplicates\"\"\"\n",
    "    \n",
    "    if len(sols) == 1:\n",
    "        return [tuple(x) for x in sols[0]]\n",
    "    \n",
    "    out_lst = []\n",
    "    for sol in sols:\n",
    "        temp = []\n",
    "        for row in sol:\n",
    "            temp.append(tuple(row))\n",
    "        out_lst.append(temp)\n",
    "    # print(out_lst)\n",
    "\n",
    "    fixed_sols = []\n",
    "    for row in out_lst[0]:\n",
    "        count = 0\n",
    "        for sol in out_lst[1:]:\n",
    "            if row in sol:\n",
    "                sol.remove(row)\n",
    "                count += 1\n",
    "            # else:\n",
    "            #     break\n",
    "        if count == len(sols) - 1:\n",
    "            fixed_sols.append(row)\n",
    "    return fixed_sols\n",
    "\n",
    "def total_sim(sol,db):\n",
    "    count = 0\n",
    "    for i, val_i in enumerate(db):\n",
    "        for j, val_j in enumerate(val_i):\n",
    "            # print(val_j, sol[i][j])\n",
    "            if val_j != sol[i][j]:\n",
    "                count += 1\n",
    "            \n",
    "    return  1 - count/(len(db[0])*len(db))\n",
    "\n",
    "def gen_cij(db,sol,i,j):\n",
    "    \"\"\" computes cost of assigning row i of solution \n",
    "    to row j of database in terms of L1 norm\"\"\"\n",
    "    \n",
    "    cost = sum([abs(db[j][k]-sol[i][k]) for k in range(len(sol[0]))])\n",
    "    return cost\n",
    "\n",
    "def ass_ILP(db,sol):\n",
    "    \"\"\" Creates an optimal assignment of rows in the solution \n",
    "    to the rows in the true database, the objective function value\n",
    "    is returned and is the smallest number of differences between\n",
    "    solution and true database, based on row swapping\"\"\"\n",
    "    \n",
    "    n = len(sol)\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "               for j in range(n)] for i in range(n)])\n",
    "    \n",
    "    for i in range(n):\n",
    "        M.addConstr(sum(x[i,:]) == 1)\n",
    "        M.addConstr(sum(x[:,i]) == 1)\n",
    "        \n",
    "    M.setObjective(sum([sum([x[i][j]*gen_cij(db,sol,i,j) for j in range(n)]) for i in range(n)]), GRB.MINIMIZE)\n",
    "    \n",
    "    M.optimize()\n",
    "    \n",
    "    out_x = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            out_x[i][j] = x[i][j].Xn\n",
    "    # print(out_x)\n",
    "    \n",
    "    return 1 - M.ObjVal/(len(db)*len(db[0]))\n",
    "\n",
    "def get_counts_uniform_seed(db,quers,noise,quer_seed_dict):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + quer2noise_uniform(query,quer_seed_dict,noise),0)\n",
    "        # print(np.random.randint(-noise,noise+1))\n",
    "    return out_dict\n",
    "\n",
    "def get_counts_triangle_seed(db,quers,noise,quer_seed_dict):\n",
    "    \"\"\"Returns noisy count of query, noise is symmetric rounded triangle [-noise, +noise]\n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + quer2noise_triangle(query,quer_seed_dict,noise),0)\n",
    "        # print(np.random.randint(-noise,noise+1))\n",
    "    return out_dict\n",
    "\n",
    "def get_counts_triangle(db,quers,noise):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        if noise == 0:\n",
    "            actual_noise = 0\n",
    "        else:\n",
    "            actual_noise = round(np.random.triangular(-noise, 0, noise))\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + actual_noise,0)\n",
    "    return out_dict\n",
    "\n",
    "def quer2noise_uniform(query,quer_seed_dict,noise):\n",
    "    \"\"\" generates noise from discrete-RV: U({-noise,noise})\n",
    "    using the seed in the query dict\"\"\"\n",
    "    # print(query)\n",
    "    if quer_seed_dict != None:\n",
    "        np.random.seed(quer_seed_dict[query])\n",
    "    return np.random.randint(-noise,noise+1)\n",
    "\n",
    "def quer2noise_triangle(query,quer_seed_dict,noise):\n",
    "    \"\"\" generates noise from rounded triangle distribution over -noise to +noise, median at 0,\n",
    "    using the seed in the query dict\"\"\"\n",
    "    # print(query)\n",
    "    if quer_seed_dict != None:\n",
    "        np.random.seed(quer_seed_dict[query])\n",
    "    return round(np.random.triangular(-noise, 0, noise))\n",
    "\n",
    "def gen_powerset_test(k):\n",
    "    \"\"\" generates a powerset of k elements \"\"\"\n",
    "    out = []\n",
    "    for i in itertools.product([0,1],repeat=k):\n",
    "        out.append(i)\n",
    "    return out\n",
    "\n",
    "def flipped_choice(choice):\n",
    "    \"\"\" returns the compliment of a query\"\"\"\n",
    "    return tuple([abs(1-x) for x in choice])\n",
    "\n",
    "def gen_queries_compliment(m,n_queries):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            used = []\n",
    "            for choice in gen_powerset(i):\n",
    "                if choice not in used:\n",
    "                    quer = []\n",
    "                    used.append(flipped_choice(choice))\n",
    "                    quer.append(comb)\n",
    "                    quer.append(choice)\n",
    "                    queries.append(tuple(quer))\n",
    "            \n",
    "            \n",
    "    sampled = sample(queries,min(int(n_queries/2),len(queries)))\n",
    "    sample_with_compliment = list(sampled)\n",
    "    for quer in sampled:\n",
    "        sample_with_compliment.append((quer[0],flipped_choice(quer[1])))  \n",
    "    \n",
    "    return sample_with_compliment\n",
    "\n",
    "def query_seed_dict(m):\n",
    "    \"\"\" assigns a seed to every possible query based on the m attributes\"\"\"\n",
    "    quer_dict = defaultdict()\n",
    "    queries = gen_queries_uniform_complete(m,max_quers(m))\n",
    "    for query in queries:\n",
    "        quer_dict[query] = int(np.random.uniform()*100000)\n",
    "    # print(quer_dict)\n",
    "    return quer_dict\n",
    "\n",
    "def max_quers(m):\n",
    "    \"\"\"returns the maximum number of queries possible from m binary attributes\"\"\"\n",
    "    tot = 0\n",
    "    for i in range(1,m+1):\n",
    "        tot += (math.factorial(m)/(math.factorial(i)*math.factorial(m-i)))*(2**i)\n",
    "    return int(tot)\n",
    "\n",
    "def gen_query_row_pair(N_q_start,n,m,c,N_sols):\n",
    "    \"\"\" creates a database, and returns pairs of backbone solutions and the additional queries \n",
    "    (and their values) it took to find that backbone solution. Starts with N_q_start queries\n",
    "    and considers N_sols to create the backbone solutions\"\"\"\n",
    "    \n",
    "    max_quer = max_quers(m)\n",
    "    print(max_quer)\n",
    "    noise = c\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    display(db)\n",
    "    \n",
    "    # might not need to be shuffled, just making sure\n",
    "    quers = gen_queries_comp(m)\n",
    "    np.random.shuffle(quers)\n",
    "    quer_seed_dict = query_seed_dict(m)\n",
    "    quer_dict_all = get_counts_uniform_seed(db,quers,noise,quer_seed_dict)\n",
    "\n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    return_lst = []\n",
    "    backbones = []\n",
    "    fixed_sols =[]\n",
    "    quer_set = set()\n",
    "\n",
    "    N_q = min(int(N_q_start),max_quer)\n",
    "    dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n",
    "    \n",
    "    while (len(backbones) != n) and (N_q != max_quer+1):\n",
    "        \n",
    "        queries = quers[0:N_q]\n",
    "        quer_dict = dict((k, quer_dict_all[k]) for k in queries)\n",
    "        sols_all = gen_sols(n,m,quer_dict,noise,N_sols)\n",
    "        sols = np.array(sols_all[0], dtype=int) \n",
    "        \n",
    "        if len(sols) < N_sols:\n",
    "            fixed_sols = check_fixed_sols(sols)\n",
    "            \n",
    "        if len(fixed_sols) > len(backbones):\n",
    "\n",
    "            fs_c = Counter(fixed_sols)\n",
    "            b_c = Counter(backbones)\n",
    "            if sum(fs_c.values()) > sum(b_c.values()):\n",
    "                new_bones = [x for x in fs_c-b_c]\n",
    "            crit_query = {queries[N_q-1]: quer_dict[queries[N_q-1]]}\n",
    "            return_lst.append((new_bones,crit_query,dictfilt(quer_dict,set(queries).difference(quer_set))))\n",
    "            backbones += new_bones\n",
    "            quer_set.update(set(queries))\n",
    "        N_q += 1\n",
    "        \n",
    "    if N_q == max_quer -1:\n",
    "        print(\"Used all queries\")\n",
    "    return return_lst\n",
    "        \n",
    "def get_results_old(c,n,m,N_q,N_sols,dist,n_part,m_part):\n",
    "    \"\"\" Returns some performance metrics for a randomly generated database, with random noise\"\"\"   \n",
    "    noise = c\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    # print(db)\n",
    "    quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "    if n_part == 0 or m_part == 0:\n",
    "        partial_dict = None\n",
    "    else:\n",
    "        partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "    \n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform(db,quers,noise)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle(db,quers,noise)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "    \n",
    "    \n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    # print(db_tup)\n",
    "\n",
    "    #solving:\n",
    "    new_n = n \n",
    "    \n",
    "    sols_all = gen_sols(new_n,m,quer_dict,partial_dict,noise,N_sols)\n",
    "    sols = np.array(sols_all[0], dtype=int) \n",
    "    time = sols_all[1] \n",
    "    \n",
    "    backbone_size = 0\n",
    "    if len(sols) < N_sols:\n",
    "        fixed_sols = check_fixed_sols(sols)\n",
    "        backbone_size = len(fixed_sols)/n\n",
    "    \n",
    "    row_sims = []\n",
    "    tot_sims = []\n",
    "    counter_sols = []\n",
    "    \n",
    "    for sol in sols:\n",
    "        sol_tup = [tuple(x) for x in sol]\n",
    "        # print(sol_tup)\n",
    "        counter_sols.append(Counter(list(sol_tup)))\n",
    "        sol_tup, common = sol_tup[:], [ e for e in db_tup if e in sol_tup and (sol_tup.pop(sol_tup.index(e)) or True)]\n",
    "        # tot_sims.append(ass_ILP(db_tup,sol_tup))\n",
    "        row_sims.append(len(common)/n)\n",
    "        \n",
    "    crit_query_count = 0\n",
    "    query_items = quer_dict.items()\n",
    "    for query in gen_crit_queries(db_tup,m,c):\n",
    "        if query in query_items:\n",
    "            crit_query_count += 1\n",
    "    \n",
    "    returns = {\n",
    "        'maximum queries': max_quers(m),\n",
    "        'backbone_size': backbone_size, \n",
    "        # 'tot_avg': np.mean(tot_sims), \n",
    "        # 'tot_min': np.min(tot_sims),\n",
    "        'row_avg': np.mean(row_sims), \n",
    "        # 'row_min': np.min(row_sims), \n",
    "        'solve_time': time, \n",
    "        'N_sols_actual': len(sols),\n",
    "        \"Contained\": Counter(db_tup) in counter_sols,\n",
    "        \"crit_queries\": crit_query_count\n",
    "    }\n",
    "    # print(counter_sols)\n",
    "    return returns\n",
    "\n",
    "def get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict):\n",
    "    \"\"\" Returns some performance metrics for a randomly generated database, with random noise....\n",
    "    \n",
    "    db: is database as a pandas array\n",
    "    seed_dict: a dictionary assigning a seed to every possible query\n",
    "    quers: a bunch of queries in a two-tuple form, will get turned into a noisy count based on 'db', 'dist' and 'seed_dict'\n",
    "    c: bound on noise, integer\n",
    "    n: size of database (eventually want this to get rid of this...)\n",
    "    m: number of attributes\n",
    "    N_sols: number of solutions to enumerate when solving\n",
    "    dist: rn either 'uniform' or 'triangle', the distribution of the noise\n",
    "    partial_dict: dictionary containing partial information.\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    \n",
    "    noise = c\n",
    "#     # print(db)\n",
    "#     quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "#     if n_part == 0 or m_part == 0:\n",
    "#         partial_dict = None\n",
    "#     else:\n",
    "#         partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "\n",
    "\n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform_seed(db,quers,noise,quer_seed_dict)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle_seed(db,quers,noise,quer_seed_dict)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "\n",
    "    \n",
    "    \n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    # print(db_tup)\n",
    "\n",
    "    #solving:\n",
    "    new_n = n \n",
    "    \n",
    "    sols_all = gen_sols(new_n,m,quer_dict,partial_dict,noise,N_sols)\n",
    "    sols = np.array(sols_all[0], dtype=int) \n",
    "    time = sols_all[1] \n",
    "    \n",
    "    backbone_size = 0\n",
    "    if len(sols) < N_sols:\n",
    "        fixed_sols = check_fixed_sols(sols)\n",
    "        backbone_size = len(fixed_sols)/n\n",
    "    \n",
    "    row_sims = []\n",
    "    tot_sims = []\n",
    "    counter_sols = []\n",
    "    \n",
    "    \n",
    "    sol_set = set()\n",
    "    for sol in sols:\n",
    "        sol_tup = [tuple(x) for x in sol]\n",
    "        sol_set.add(tuple(sol_tup))\n",
    "        # print(sol_tup)\n",
    "        counter_sols.append(Counter(list(sol_tup)))\n",
    "        sol_tup, common = sol_tup[:], [ e for e in db_tup if e in sol_tup and (sol_tup.pop(sol_tup.index(e)) or True)]\n",
    "        # tot_sims.append(ass_ILP(db_tup,sol_tup))\n",
    "        row_sims.append(len(common)/n)\n",
    "        \n",
    "    crit_query_count = 0\n",
    "    query_items = quer_dict.items()\n",
    "    for query in gen_crit_queries(db_tup,m,c):\n",
    "        if query in query_items:\n",
    "            crit_query_count += 1\n",
    "            \n",
    "    \n",
    "    # print(len(sol_set))\n",
    "    # print(sols)\n",
    "    returns = {\n",
    "        'maximum queries': max_quers(m),\n",
    "        'backbone_size': backbone_size, \n",
    "        # 'tot_avg': np.mean(tot_sims), \n",
    "        # 'tot_min': np.min(tot_sims),\n",
    "        'row_avg': np.mean(row_sims), \n",
    "        # 'row_min': np.min(row_sims), \n",
    "        'solve_time': time, \n",
    "        'N_sols_actual': len(sols),\n",
    "        \"Contained\": Counter(db_tup) in counter_sols,\n",
    "        \"crit_queries\": crit_query_count\n",
    "    }\n",
    "    # print(counter_sols)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def gen_crit_queries(db,m,c):\n",
    "    outputs = []\n",
    "    A = tuple(range(m))\n",
    "    for row in db:\n",
    "        V = tuple(row)\n",
    "        for i in range(3):\n",
    "            outputs.append(((A,V),c+i))\n",
    "    return outputs\n",
    "\n",
    "def compare_noise_types(n,m,c,N_q,N_sols,N_trials,n_part,m_part):\n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    tri_list = []\n",
    "    uni_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    quer_seed_dict = None\n",
    "    \n",
    "    for i in range(N_trials):\n",
    "        for dist in ['triangle','uniform']:\n",
    "            quers = gen_queries_uniform_complete(m,N_q)\n",
    "            db = gen_bin_data_set(n,m)\n",
    "            partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "            \n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist)\n",
    "            results[dist].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[dist] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for dist in ['triangle','uniform']:\n",
    "        df = pd.DataFrame(results[dist])\n",
    "        out_results[dist] = dict(df.mean()) \n",
    "        out_results[dist]['frac_solved_uniqely'] = unique_counts[dist]/N_trials\n",
    "        out_results[dist]['params'] = {'n': n, 'm': m, 'c': c, 'N_q': N_q, 'N_sols': N_sols, 'N_trials': N_trials}\n",
    "    return out_results\n",
    "\n",
    "def gen_queries_count_size(db,c,dist,quer_seed_dict,N_q,first):\n",
    "    quers_all = gen_queries_uniform_complete(m,max_quers(m))\n",
    "    if dist == 'uniform': quer_counts = get_counts_uniform_seed(db,quers_all,c,quer_seed_dict)\n",
    "    if dist == 'triangle': quer_counts = get_counts_triangle_seed(db,quers_all,c,quer_seed_dict)\n",
    "    \n",
    "    if first == 'large':\n",
    "        sorted_quers = {k: v for k, v in sorted(quer_counts.items(), key=lambda item: item[1], reverse = True)}\n",
    "    elif first == 'small':\n",
    "        sorted_quers = {k: v for k, v in sorted(quer_counts.items(), key=lambda item: item[1])}\n",
    "        \n",
    "    first_Nq_quers = {k: sorted_quers[k] for k in list(sorted_quers.keys())[0:N_q]}\n",
    "    # print(first_Nq_quers)\n",
    "    return list(first_Nq_quers.keys())\n",
    "\n",
    "def gen_queries(m,n,db,c,quer_seed_dict,N_q,quer_type,dist):\n",
    "    \n",
    "    N_q = min(N_q,max_quers(m))\n",
    "    all_quers = gen_queries_all(m)\n",
    "    \n",
    "    if quer_type == 'uniform_random':\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "    elif quer_type == 'large_counts_first':\n",
    "        quers = gen_queries_count_size(db,c,dist,quer_seed_dict,N_q,'large')\n",
    "        \n",
    "    elif quer_type == 'small_counts_first':\n",
    "        quers = gen_queries_count_size(db,c,dist,quer_seed_dict,N_q,'small')\n",
    "        \n",
    "    elif quer_type == 'many_atts_first':\n",
    "        quers = sorted(all_quers,key =lambda item: len(item[0]), reverse = True)[0:N_q]\n",
    "    \n",
    "    elif quer_type == 'few_atts_first':\n",
    "        quers = sorted(all_quers,key =lambda item: len(item[0]))[0:N_q]\n",
    "        \n",
    "    return quers\n",
    "        \n",
    "    \n",
    "\n",
    "def compare_query_types(n,m,c,N_q,N_sols,N_trials,dist,quer_types):\n",
    "    \n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    normal_list = []\n",
    "    partial_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        # quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        for quer_type in quer_types:\n",
    "            # partial_dict = gen_partial_info(db,partial[0],partial[1])\n",
    "            # print(partial_dict)\n",
    "            \n",
    "            partial_dict = dict()\n",
    "            quers = gen_queries(m,n,db,c,quer_seed_dict,N_q,quer_type,dist)\n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist,partial[0],partial[1])\n",
    "            results[quer_type].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[quer_type] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for quer_type in quer_types:\n",
    "        df = pd.DataFrame(results[quer_type])\n",
    "        out_results[quer_type] = dict(df.mean()) \n",
    "        out_results[quer_type]['frac_solved_uniqely'] = unique_counts[quer_type]/N_trials\n",
    "        out_results[quer_type]['params'] = {'n': int(n), 'm': int(m), 'c': int(c), 'N_q': int(N_q), \n",
    "                                               'N_sols': int(N_sols), 'N_trials': int(N_trials)}\n",
    "        # out_results[]\n",
    "        \n",
    "    return out_results\n",
    "\n",
    "def compare_partial_info(n,m,c,N_q,N_sols,N_trials,dist,partials):\n",
    "    \"\"\"partials is a list of 2-element lists, where the first position is n_part,\n",
    "    the number of people to generate partial info from, and second is m_part, \n",
    "    the number of attributes for each person to 'know'. Comapres each 2-element list, \n",
    "    asking N_q queries on each, averaging N_trials times\n",
    "    \n",
    "    should this all be done with additive query style? \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    normal_list = []\n",
    "    partial_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        for partial in partials:\n",
    "            partial_dict = gen_partial_info(db,partial[0],partial[1])\n",
    "            # print(partial_dict)\n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist,partial[0],partial[1])\n",
    "            results[str(partial)].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[str(partial)] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for partial in partials:\n",
    "        df = pd.DataFrame(results[str(partial)])\n",
    "        out_results[str(partial)] = dict(df.mean()) \n",
    "        out_results[str(partial)]['frac_solved_uniqely'] = unique_counts[tuple(partial)]/N_trials\n",
    "        out_results[str(partial)]['params'] = {'n': int(n), 'm': int(m), 'c': int(c), 'N_q': int(N_q), \n",
    "                                               'N_sols': int(N_sols), 'N_trials': int(N_trials)}\n",
    "        # out_results[]\n",
    "    return out_results\n",
    "\n",
    "def single_basic_run(n,m,c,N_q,N_sols,N_trials,dist,n_part,m_part):\n",
    "    \"\"\" Generates a dataset and solves, averaging over N_trials times.\n",
    "    returns a dictionary with all the performance 'metrics'\"\"\"\n",
    "    \n",
    "    out_lst = []\n",
    "    unique_counts = 0\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        # print(db)\n",
    "        partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "        # print(partial_dict)\n",
    "        temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "        out_lst.append(temp_res)\n",
    "        if temp_res['N_sols_actual'] == 1:\n",
    "            unique_counts += 1\n",
    "            \n",
    "    df = pd.DataFrame(out_lst)\n",
    "    out_dict = dict(df.mean()) \n",
    "    out_dict['frac_solved_uniqely'] = unique_counts/N_trials\n",
    "\n",
    "    return out_dict\n",
    "    \n",
    "def compare_query_types_additive(n,m,c,N_q_range,N_sols,N_trials,dist,quer_types):\n",
    "    \n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict()\n",
    "    out_lst = []\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        quers_all = defaultdict()\n",
    "        for quer_type in quer_types:\n",
    "            quers_all[quer_type] = gen_queries(m,n,db,c,quer_seed_dict,max_quers(m),quer_type,dist)\n",
    "            \n",
    "        partial_dict = dict()\n",
    "        for N_q_try in N_q_range:\n",
    "            N_q = int(min(N_q_try,max_quers(m)))\n",
    "\n",
    "            temp_dict = defaultdict()\n",
    "            \n",
    "            for quer_type in quer_types:\n",
    "                \n",
    "                quers = quers_all[quer_type][0:N_q]\n",
    "                temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "                temp_dict[quer_type] = temp_res\n",
    "                \n",
    "            results[N_q] = temp_dict\n",
    "            \n",
    "        out_lst.append(results)\n",
    "    return ave(out_lst)\n",
    "\n",
    "def ave(d):\n",
    "    \"\"\"given a list of nested dictionaries, it will return a dictionary of averages \n",
    "    in the same format as the individual dicts\"\"\"\n",
    "    _data = sorted([i for b in d for i in b.items()], key=lambda x:x[0])\n",
    "    _d = [(a, [j for _, j in b]) for a, b in groupby(_data, key=lambda x:x[0])]\n",
    "    return {a:ave(b) if isinstance(b[0], dict) else sum(b)/float(len(b)) for a, b in _d}\n",
    "\n",
    "def get_quer_counts(db,m,c,quer_seed_dict,dist):\n",
    "    quers = quer_seed_dict.keys()\n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform_seed(db,quers,c,quer_seed_dict)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle_seed(db,quers,c,quer_seed_dict)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "    return quer_dict\n",
    "\n",
    "def path2filename(path):\n",
    "    return path.lstrip(os.getcwd())\n",
    "\n",
    "def my_add_nodes(paths_used,network,res_dict,numbering,node_scaling):\n",
    "    # net.add_node('a', hidden = True, value = 0)\n",
    "    path_nodes = Counter()\n",
    "    # for i in range(min(len(res_dict.values()),paths_used)):\n",
    "    for i in res_dict.keys():\n",
    "        path_nodes.update(res_dict[str(i)][0])\n",
    "        # path_nodes.update([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "    # print(path_nodes)\n",
    "    not_visited = list(set(numbering.values()) - set(path_nodes.keys()))\n",
    "    for query in path_nodes.keys():\n",
    "        net.add_node(query, str(query),\n",
    "                     size=node_scaling*path_nodes[query], \n",
    "                     # value=50, \n",
    "                     color = 'blue', \n",
    "                     title= str(numbering.inverse[query])+ \"\\n\"+ \"visits: \" + str(path_nodes[query])\n",
    "                 # level = len(query[0])\n",
    "                )\n",
    "    for query in not_visited:\n",
    "        net.add_node(query, size=10, color = 'green', title = str(numbering.inverse[query])\n",
    "                 # level = len(query[0])\n",
    "                )\n",
    "    return None\n",
    "\n",
    "def my_add_edges(paths_used,res_dict,colour,network):\n",
    "    # network.add_edge(1,1,value = 0, hidden = True)\n",
    "    # for j in range(min(len(res_dict.values()),paths_used)):\n",
    "    for j in res_dict.keys():\n",
    "        path = res_dict[str(j)][0]\n",
    "        temp_col = \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "        # print(temp_col)\n",
    "        for i in range(len(path) - 1):\n",
    "            # np.random.seed(np.random.choice(range(100000)))\n",
    "            network.add_edge(int(path[i]),int(path[i+1]), color = temp_col, size =0.1)\n",
    "    return None\n",
    "\n",
    "def gen_paths_SA(numbering,N_trials,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k):\n",
    "    \n",
    "    results = defaultdict()\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    for i in range(N_trials):\n",
    "        if i%10 ==1:\n",
    "            print(i)\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "        results[i] = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def gen_paths_SA_additive_k(numbering,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k_low,k_high):\n",
    "    \n",
    "    results = defaultdict()\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    quer_seed_dict = query_seed_dict(m)\n",
    "    quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "    for k in range(k_low,k_high+1):\n",
    "        if k%5 ==1:\n",
    "            print(k)\n",
    "        results[k] = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def genetic_algo(n_iters, pop_size,n_parents, n_crossovers, n_mutants, quer_counts,n,m,c,N_sols,k,numbering):\n",
    "    largest_number = max_quers(m)\n",
    "    initial_pop = [np.random.choice(list(range(largest_number)), k, replace = False) for x in range(pop_size)]\n",
    "    fit_dict = defaultdict()\n",
    "    for i in initial_pop:\n",
    "        fit_dict[tuple(i)] = get_fitness(i,n,m,c,quer_counts,N_sols,numbering)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        if len(set(fit_dict.keys())) <= 3:\n",
    "            return list(set(fit_dict.keys()))[0], fit_dict[list(set(fit_dict.keys()))[0]]\n",
    "        parents = choose_parents(fit_dict,n_parents)\n",
    "        children = breed(parents,n_crossovers,largest_number)\n",
    "        pop = parents+children\n",
    "        pop += mutate(pop,n_mutants,largest_number)\n",
    "        new_fit_dict = cull(pop,fit_dict,pop_size,n,m,c,quer_counts,N_sols,numbering)\n",
    "        fit_dict = dict(new_fit_dict)\n",
    "        \n",
    "        ####this is a shitty fix - change this\n",
    "        if len(set(fit_dict.values())) == 1:\n",
    "            break\n",
    "    \n",
    "    min_fit = min(fit_dict.values())\n",
    "    for i in fit_dict.keys():\n",
    "        if fit_dict[i] == min_fit:\n",
    "            return sorted(i), fit_dict[i]\n",
    "    \n",
    "def get_fitness(sol,n,m,c,quer_counts,N_sols,numbering):\n",
    "    quers = []\n",
    "    for num in sol:\n",
    "        quers.append(numbering.inverse[num])\n",
    "    return get_n_sols(n,m,c,quers,quer_counts,N_sols)\n",
    "\n",
    "def choose_parents(fit_dict,n_parents):\n",
    "    choices = tuple(fit_dict.keys())\n",
    "    n_sol_lst = []\n",
    "    tot = sum(fit_dict.values())\n",
    "    for choice in choices:\n",
    "        n_sol_lst.append((fit_dict[choice]))\n",
    "    if len(set(n_sol_lst)) == 1:\n",
    "        probs = np.array([1/len(n_sol_lst)]*len(n_sol_lst))\n",
    "    else:\n",
    "        probs = []\n",
    "        max_lst = max(n_sol_lst)\n",
    "        for i in n_sol_lst:\n",
    "            prob = (max_lst - i)/sum([max_lst - x for x in n_sol_lst])\n",
    "            if prob == 0:\n",
    "                prob += 0.001\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "    probs /= probs.sum()\n",
    "    # print(probs)\n",
    "    choices_index = np.random.choice(range(len(choices)), n_parents, replace=False, p=probs)\n",
    "    return [choices[i] for i in choices_index]\n",
    "\n",
    "def breed(parents,n_crossovers,largest_number):\n",
    "    children = []\n",
    "    for i in range(n_crossovers):\n",
    "        choice_index = np.random.choice(range(len(parents)), 2, replace= False)\n",
    "        couple = [parents[i] for i in choice_index]\n",
    "        children += crossover(couple,largest_number)\n",
    "    return children \n",
    "\n",
    "def crossover(couple,largest_number):\n",
    "    parent1 = couple[0]\n",
    "    parent2 = couple[1]\n",
    "    point = np.random.randint(0,len(couple[0])+1)\n",
    "    child1 = parent1[0:point] + parent2[point:]\n",
    "    child2 = parent2[0:point] + parent1[point:]\n",
    "    return [fix_child(child1,largest_number), fix_child(child2,largest_number)]\n",
    "    \n",
    "def fix_child(child,largest_number):\n",
    "    child_set = set(child)\n",
    "    new_choices = np.random.choice(list(set(range(largest_number))-child_set),len(child) - len(child_set), replace= False )\n",
    "    return list(child_set.union(new_choices))\n",
    "\n",
    "def cull(pop,fit_dict,pop_size,n,m,c,quer_counts,N_sols,numbering):\n",
    "    for i_ in pop:\n",
    "        i = tuple(i_)\n",
    "        if i not in fit_dict.keys():\n",
    "            fit_dict[i] = get_fitness(i,n,m,c,quer_counts,N_sols,numbering)\n",
    "    sorted_pop = list(sorted(fit_dict.items(), key=lambda x:x[1]))\n",
    "    culled_pop = [sorted_pop[x][0] for x in range(pop_size)]\n",
    "    new_fit_dict = defaultdict()\n",
    "    for i in culled_pop:\n",
    "        new_fit_dict[i] = fit_dict[i]\n",
    "    return new_fit_dict\n",
    "\n",
    "def mutate(pop,n_mutants,largest_number):\n",
    "    mutant_choice = np.random.choice(range(len(pop)), n_mutants)\n",
    "    mutated = []\n",
    "    for choice in mutant_choice:\n",
    "        ind = pop[choice]\n",
    "        bit_choice = np.random.choice(range(len(ind)))\n",
    "        new_ind = list(ind)\n",
    "        new_quer = int(np.random.choice(list(set(range(largest_number))-set(ind))))\n",
    "        new_ind[bit_choice] = new_quer\n",
    "        mutated.append(new_ind)\n",
    "    return mutated\n",
    "\n",
    "def simu_annealing(n_iters,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering):\n",
    "    \n",
    "    largest_number = max_quers(m)\n",
    "    fit_dict = defaultdict()\n",
    "    current_sol = tuple(np.random.choice(list(range(largest_number)), k, replace = False))\n",
    "    fit_dict['current_sol'] = get_fitness(current_sol,n,m,c,quer_counts,N_sols,numbering)\n",
    "    temp = t_0\n",
    "    last_ = [i for i in range(repeats)]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        if i >= n_iters*0.5:\n",
    "            last_[i%repeats] = int(fit_dict['current_sol'])\n",
    "            if len(set(last_)) == 1:\n",
    "                # print(i)\n",
    "                break\n",
    "        new_sol = find_neighbour(current_sol,largest_number)\n",
    "        fit_dict['new_sol'] = get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering)\n",
    "        # print(fit_dict)\n",
    "        change = fit_dict['new_sol'] - fit_dict['current_sol']\n",
    "        # print(change)\n",
    "        p = get_prob(temp,change)\n",
    "        # print(p)\n",
    "        if np.random.uniform() < p:\n",
    "            current_sol = tuple(new_sol)\n",
    "            fit_dict['current_sol'] = int(fit_dict['new_sol'])\n",
    "        temp = temp*alpha\n",
    "    # print(i)\n",
    "    return (tuple(sorted([int(x) for x in current_sol])), get_fitness(current_sol,n,m,c,quer_counts,N_sols,numbering))\n",
    "        \n",
    "def get_prob(temp,change):\n",
    "    if change <= 0:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = (math.e)**(-change/temp)\n",
    "    return p\n",
    "            \n",
    "def find_neighbour(sol,largest_number):\n",
    "    bit_choice = np.random.choice(range(len(sol)))\n",
    "    new_ind = list(sol)\n",
    "    new_quer = int(np.random.choice(list(set(range(largest_number))-set(sol))))\n",
    "    new_ind[bit_choice] = new_quer\n",
    "    return tuple(new_ind)\n",
    "\n",
    "def produce_paths(db,quer_seed_dict,n,m,c,dist,N_sols,k,N_paths,numbering):\n",
    "    \"\"\"k is path length\"\"\"\n",
    "    \n",
    "    partial_dict = dict()\n",
    "    all_quers = list(quer_seed_dict.keys()) \n",
    "\n",
    "    paths = []\n",
    "    sols_path_dict = defaultdict()\n",
    "    backbone_path_dict = defaultdict()\n",
    "    for i in range(N_paths):\n",
    "        path = tuple(np.random.choice(list(numbering.inverse.keys()),min(k,max_quers(m)), replace = False))\n",
    "        paths.append(path)\n",
    "    for path in paths:\n",
    "        quers = [numbering.inverse[x] for x in path]\n",
    "        res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "        sols_path_dict[path] = res['N_sols_actual']\n",
    "        backbone_path_dict[path] = res['backbone_size']\n",
    "        \n",
    "    return sols_path_dict, backbone_path_dict\n",
    "    \n",
    "def good_paths(path_dicts, numbering, sol_thresh, bb_thresh):\n",
    "    \"\"\"a path is a good path if it has leq sol_thresh number of feasible solutions, \n",
    "    or if it has geq bb_thresh backbone fraction.\"\"\"\n",
    "    \n",
    "    good_paths = []\n",
    "    good_quer_count = Counter()\n",
    "    quer_count = Counter()\n",
    "\n",
    "    for path in path_dicts[0].keys():\n",
    "        quer_count.update(path)\n",
    "        if path_dicts[0][path] <= sol_thresh or path_dicts[1][path] >= bb_thresh:\n",
    "            good_paths.append(path)\n",
    "            good_quer_count.update(path)\n",
    "    return good_paths, good_quer_count\n",
    "\n",
    "def quer_perf_avg(path_dicts,numbering):\n",
    "\n",
    "    quer_score_dict = defaultdict(lambda: 0)\n",
    "    quer_count = Counter()\n",
    "    # print(path_dicts)\n",
    "    \n",
    "    for path in path_dicts[0].keys():\n",
    "        quer_count.update(path)\n",
    "        for quer in path:\n",
    "            # print(quer)\n",
    "            quer_score_dict[quer] += path_dicts[0][path]\n",
    "    for quer in quer_score_dict.keys():\n",
    "        quer_score_dict[quer] /= quer_count[quer]\n",
    "    return quer_score_dict\n",
    "\n",
    "def get_n_sols(n,m,c,quers,quer_counts,N_sols):\n",
    "    \"\"\"returns number of feasible there are for choice of queries, where\n",
    "    info about database + noisy counts is given in quer_counts\"\"\"\n",
    "    quer_dict = defaultdict()\n",
    "    for quer in quers:\n",
    "        quer_dict[quer] = quer_counts[quer]\n",
    "    return n_sols(n,m,quer_dict,c,N_sols)\n",
    "\n",
    "def produce_count(dist,quer_seed_dict,db,quer,c):\n",
    "    quers = [quer]\n",
    "    if dist == 'uniform':\n",
    "            quer_dict = get_counts_uniform_seed(db,quers,c,quer_seed_dict)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle_seed(db,quers,c,quer_seed_dict)\n",
    "    return list(quer_dict.values())[0]\n",
    "\n",
    "def sort_quer(quer):\n",
    "    vals = [x for _,x in sorted(zip(quer[0],quer[1]))]\n",
    "    return (tuple(sorted(quer[0])),tuple(vals))\n",
    "\n",
    "def gen_queries_algo(db,m,quer_seed_dict,dist,c,N_q):\n",
    "    \n",
    "    quers = defaultdict()\n",
    "    seeds = np.random.randint(0,100000,1000)\n",
    "    i=0\n",
    "    \n",
    "    while len(quers.keys()) < min(max_quers(m),N_q):\n",
    "        \n",
    "        np.random.seed(seeds[i])\n",
    "        order = list(np.random.choice(list(range(m)),m, replace = False))\n",
    "        # print(order)\n",
    "        A = []\n",
    "        V = []\n",
    "        for i in order:\n",
    "            A.append(i)\n",
    "            quer0 = sort_quer([A,V+[0]])\n",
    "            quer1 = sort_quer([A,V+[1]])\n",
    "            \n",
    "            val0 = produce_count(dist,quer_seed_dict,db,quer0,c)\n",
    "            val1 = produce_count(dist,quer_seed_dict,db,quer1,c)\n",
    "            \n",
    "            # print(val0)\n",
    "            \n",
    "            if quer0 in quers.keys():\n",
    "                V+= [1]\n",
    "            elif quer1 in quers.keys():\n",
    "                V+= [0]\n",
    "            else:\n",
    "                V += [np.argmax([val0,val1])]\n",
    "            quers[quer0] = val0\n",
    "            quers[quer1] = val1\n",
    "            # print(A,V)\n",
    "        i+=1\n",
    "        \n",
    "    return quers\n",
    "\n",
    "def produce_alt_paths(sol,T_max,n,m,c,quer_counts,N_sols,numbering):\n",
    "    largest_number = max_quers(m)\n",
    "    current_n_sols = get_fitness(sol,n,m,c,quer_counts,N_sols,numbering)\n",
    "    alt_sols = []\n",
    "    pop = [list(sol)]\n",
    "    curr_time = t.time()\n",
    "    while t.time() <= curr_time + T_max:\n",
    "        new_sol = mutate(pop,1,largest_number)[0]\n",
    "        if get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering) <= current_n_sols:\n",
    "            alt_sols.append(tuple(new_sol))\n",
    "    return set(alt_sols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23d5bec-f068-4a20-8df4-07e7f2626b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_quer_vals_ILP(c,m,n,quer_dict,target_quers,N_sols):\n",
    "    \"\"\" creates a dictionary of noisy counts based on database db\"\"\"\n",
    "\n",
    "    all_quers = gen_queries_all(m)\n",
    "    var_dict = defaultdict()\n",
    "    # true_counts = get_counts_uniform(db,all_quers,0)\n",
    "    # true_counts[((),())] = n\n",
    "    \n",
    "    # print(quer_dict[target_quers[0]])\n",
    "            \n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "#     # this is constraint for the empty query - i.e. noisy value of n\n",
    "    var_dict[((),())] = M.addVar(vtype='I', name=f\"((),())\")\n",
    "#     M.addConstr(var_dict[((),())] >= 0 )\n",
    "    M.addConstr(var_dict[((),())] ==  n)\n",
    "    # M.addConstr(var_dict[((),())] <=  n)\n",
    "    \n",
    "    for quer in all_quers:\n",
    "        var_dict[quer] = M.addVar(vtype='I', name=f\"{quer}\")\n",
    "        M.addConstr(var_dict[quer] >= 0 )\n",
    "        M.addConstr(var_dict[quer] <= n )\n",
    "        \n",
    "    for quer in quer_dict.keys():\n",
    "        # var_dict[quer] = M.addVar(vtype='I', name=f\"{quer}\")\n",
    "        # M.addConstr(var_dict[quer] >= 0 )\n",
    "        M.addConstr(var_dict[quer] >=  quer_dict[quer] - c)\n",
    "        M.addConstr(var_dict[quer] <=  quer_dict[quer] + c)\n",
    "    \n",
    "    marginal_constraints_target(M,var_dict,m)\n",
    "    \n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = N_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    \n",
    "    # print(f\"Took {M.Runtime:.2f} seconds to solve\")\n",
    "    print(f'new_ILP: {M.SolCount} solutions')\n",
    "    \n",
    "    temp_dict = defaultdict(list)\n",
    "    for k in range(M.SolCount):\n",
    "        M.Params.SolutionNumber = k\n",
    "        # var_dict_out = {x: var_dict[x].Xn - true_counts[x] for x in var_dict.keys()}\n",
    "        for quer in target_quers:\n",
    "            # print(var_dict[quer].Xn)\n",
    "            temp_dict[quer].append(round(var_dict[quer].Xn))\n",
    "        # var_dict_out = {x: var_dict[x].Xn for x in var_dict.keys()}\n",
    "        # out_lst.append(n.Xn)\n",
    "        # out_lst.append(var_dict_out)\n",
    "    out_dict = dict((quer,set(temp_dict[quer])) for quer in target_quers)\n",
    "    return out_dict\n",
    "\n",
    "def marginal_constraints_target(M,var_dict,m):\n",
    "    \"\"\" Adds the constraints based on the queries in quer_dict\"\"\"\n",
    "    \n",
    "    for quer in var_dict.keys():\n",
    "        # print(quer)\n",
    "        parts = gen_partitions_additional_attribute_target(quer,m)\n",
    "        # print(parts)\n",
    "        for part in parts:\n",
    "            M.addConstr(var_dict[tuple(part[0])] + var_dict[tuple(part[1])] == var_dict[quer])\n",
    "    return None\n",
    "\n",
    "def gen_partitions_additional_attribute_target(query,m):\n",
    "    \"\"\" returns a list of all the partitions of the query with one additional attribute\"\"\"\n",
    "    other_atts = sorted(list(set(range(m)).difference(set(query[0]))))\n",
    "    \n",
    "    lst = []\n",
    "    for i in other_atts:\n",
    "        # print(i)\n",
    "        # print(list(query[0]))\n",
    "        quer_A = list(query[0])\n",
    "        quer_A.append(i)\n",
    "        quer_A = sorted(quer_A)\n",
    "        \n",
    "        lstt = []\n",
    "        for j in [0,1]:\n",
    "            quer_B = list(query[1])\n",
    "            quer_B.insert(np.searchsorted(query[0],i),j)\n",
    "            lstt.append((tuple(quer_A),tuple(quer_B)))\n",
    "        lst.append(lstt)\n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f9a4e-5d05-44f4-89de-af57c6ae7eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum queries: 2186\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "m = 7\n",
    "c = 3\n",
    "print(f'maximum queries: {max_quers(m)}')\n",
    "dist = 'uniform'\n",
    "db = gen_bin_data_set(n,m)\n",
    "display(db)\n",
    "N_sols = 10000\n",
    "quer_seed_dict = query_seed_dict(m)\n",
    "all_quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "n_queries = 1200\n",
    "\n",
    "sample_quers = gen_queries_uniform_complete(m,n_queries)\n",
    "target_quers = [((0,), (0,)), ((0,), (1,)), (tuple([x for x in range(m)]), tuple([0 for x in range(m)]))]\n",
    "print(all_quer_counts[target_quers[0]])\n",
    "\n",
    "if target_quers[0] in sample_quers:\n",
    "    print(f'Query contained in sample, value is {all_quer_counts[target_quers[0]]}')\n",
    "\n",
    "current_quers_dict = dict((i,all_quer_counts[i]) for i in sample_quers)\n",
    "print(target_quer_vals_ILP(c,m,n,current_quers_dict,target_quers,N_sols))\n",
    "\n",
    "# print(n_sols(n,m,current_quers_dict,c,N_sols))\n",
    "# feasible_sols = gen_sols(n,m,current_quers_dict,dict(),c,N_sols)[0]\n",
    "# lst = []\n",
    "# for sol in feasible_sols:\n",
    "#     sol_db = pd.DataFrame(sol, columns=[f'att_{x}' for x in range(m)])\n",
    "#     lst.append(get_counts_uniform(sol_db,[target_quers[0]],0)[tuple([target_quers[0]][0])])\n",
    "# print(set(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c0845a-18ac-413b-aacc-331e3aa56d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[((0, 1, 2), (0, 0, 0)), ((0, 1, 2), (0, 0, 1))],\n",
       " [((0, 1, 3), (0, 0, 0)), ((0, 1, 3), (0, 0, 1))]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_quer = ((0,1),(0,0))\n",
    "m =4\n",
    "gen_partitions_additional_attribute(query,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2471ebcd-7987-465e-9012-8f0829265ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3}\n"
     ]
    }
   ],
   "source": [
    "a = set()\n",
    "print(a.union(set([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e009beb1-8501-47b4-8f7e-921ad7bbc11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[((0, 1, 2), (0, 0, 0)), ((0, 1, 2), (0, 0, 1))],\n",
       " [((0, 1, 3), (0, 0, 0)), ((0, 1, 3), (0, 0, 1))]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_partitions_additional_attribute(((0,1),(0,0)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1fcacbc1-0627-4a9e-842a-1f4969bf01e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(4.77777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4540b325-1e8e-4f98-9971-9395f5541ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1, 2), (0, 0, 0))\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "print((tuple([x for x in range(m)]), tuple([0 for x in range(m)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87881665-53eb-4f7a-98e0-a8e2baed93a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
