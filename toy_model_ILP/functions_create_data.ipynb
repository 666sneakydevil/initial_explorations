{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c29ba360-5460-4173-b3c2-461e46b15002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import io\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from itertools import combinations\n",
    "from itertools import groupby\n",
    "\n",
    "from random import sample\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"font.serif\": ['Computer Modern Roman'],\n",
    "# })\n",
    "from pyvis.network import Network\n",
    "from bidict import bidict\n",
    "import gravis as gv\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3f2d06-15c8-4e00-9dcb-b3aa4f6e3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sols_var_n(n_range,m,quer_dict,partial_dict,noise,n_sols):\n",
    "    \"\"\" builds ILP to construct candidate database, based on answers to queries \n",
    "    Assumes bound on noise, n and m is known. \n",
    "    \n",
    "    havent changed anything to this yet****************\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    # Initialize the decision variables\n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "                   for j in range(m)] for i in range(n)])\n",
    "    # print(x)\n",
    "    \n",
    "    #adding the constraints...\n",
    "    quer_cons(M,x,n,quer_dict,noise)\n",
    "    partial_cons(M,x,n,partial_dict)\n",
    "            \n",
    "    # for i in range(n-1):\n",
    "    #     M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    bin_cons(M,x,n)\n",
    "\n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = n_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    \n",
    "    # print(f\"Took {M.Runtime:.2f} seconds to solve\")\n",
    "    \n",
    "    if M.solCount == 0:\n",
    "        print(\"infeasible\")\n",
    "    \n",
    "    out_lst = []\n",
    "    for k in range(M.SolCount):\n",
    "        M.Params.SolutionNumber = k\n",
    "        out_x = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[0])):\n",
    "                out_x[i][j] = x[i][j].Xn\n",
    "        # print([var.Xn for var in m.getVars()])\n",
    "        out_lst.append(out_x)\n",
    "        \n",
    "    return out_lst, M.Runtime\n",
    "\n",
    "def n_sols(n,m,quer_dict,noise,n_sols):\n",
    "    \"\"\" builds ILP to construct candidate database, based on answers to queries \n",
    "    Assumes bound on noise, n and m is known. \"\"\"\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    # Initialize the decision variables\n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "                   for j in range(m)] for i in range(n)])\n",
    "    # print(x)\n",
    "    partial_dict = dict()\n",
    "    #adding the constraints...\n",
    "    quer_cons(M,x,n,quer_dict,noise)\n",
    "    partial_cons(M,x,n,partial_dict)\n",
    "            \n",
    "    # for i in range(n-1):\n",
    "    #     M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    bin_cons(M,x,n)\n",
    "\n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = n_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    return M.SolCount\n",
    "\n",
    "def gen_sols(n,m,quer_dict,partial_dict,noise,n_sols):\n",
    "    \"\"\" builds ILP to construct candidate database, based on answers to queries \n",
    "    Assumes bound on noise, n and m is known. \"\"\"\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    # Initialize the decision variables\n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "                   for j in range(m)] for i in range(n)])\n",
    "    # print(x)\n",
    "    \n",
    "    #adding the constraints...\n",
    "    quer_cons(M,x,n,quer_dict,noise)\n",
    "    partial_cons(M,x,n,partial_dict)\n",
    "            \n",
    "    # for i in range(n-1):\n",
    "    #     M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    bin_cons(M,x,n)\n",
    "\n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = n_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    \n",
    "    # print(f\"Took {M.Runtime:.2f} seconds to solve\")\n",
    "    \n",
    "    if M.solCount == 0:\n",
    "        print(\"infeasible\")\n",
    "    \n",
    "    out_lst = []\n",
    "    for k in range(M.SolCount):\n",
    "        M.Params.SolutionNumber = k\n",
    "        out_x = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[0])):\n",
    "                out_x[i][j] = x[i][j].Xn\n",
    "        # print([var.Xn for var in m.getVars()])\n",
    "        out_lst.append(out_x)\n",
    "        \n",
    "    return out_lst, M.Runtime\n",
    "\n",
    "def quer_cons(M,x,n,quer_dict,noise):\n",
    "    \"\"\" Adds the constraints based on the queries in quer_dict\"\"\"\n",
    "    \n",
    "    y_var_dict = defaultdict()\n",
    "    y_p_var_dict = defaultdict()\n",
    "    y_n_var_dict = defaultdict()\n",
    "    \n",
    "    for quer in quer_dict.keys():\n",
    "        y_var_dict[quer] = []\n",
    "        y_p_var_dict[quer] = []\n",
    "        y_n_var_dict[quer] = []\n",
    "        \n",
    "        positions_0 = []\n",
    "        positions_1 = []\n",
    "        for i, j in zip(quer[0],quer[1]):\n",
    "            if j == 0: \n",
    "                positions_0.append(i)\n",
    "            elif j ==1:\n",
    "                positions_1.append(i)\n",
    "        for i in range(n):\n",
    "            y_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_{quer}_{i}\"))\n",
    "            y_p_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_p_{quer}_{i}\"))\n",
    "            y_n_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_n_{quer}_{i}\"))\n",
    "            \n",
    "            M.addConstr( len(quer[0])*y_var_dict[quer][i] + (len(quer[0])+0.5)*y_p_var_dict[quer][i] <= \n",
    "                        sum([x[i][j] for j in positions_1]) + sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( (len(quer[0])-0.5)*y_n_var_dict[quer][i] + len(quer[0])*y_var_dict[quer][i] + \n",
    "                        m*y_p_var_dict[quer][i] >= \n",
    "                        sum([x[i][j] for j in positions_1])+ sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( y_var_dict[quer][i] + y_p_var_dict[quer][i] + y_n_var_dict[quer][i] == 1)\n",
    "            \n",
    "        if noise == 0:\n",
    "            M.addConstr(sum(y_var_dict[quer]) == quer_dict[quer])\n",
    "        else:\n",
    "            M.addConstr(sum(y_var_dict[quer]) <= quer_dict[quer] + noise)\n",
    "            M.addConstr(sum(y_var_dict[quer]) >= quer_dict[quer] - noise)\n",
    "    return None\n",
    "\n",
    "def partial_cons(M,x,n,partial_dict):\n",
    "    \"\"\" Adds the constraints based on the partial information in partial_dict\"\"\"\n",
    "    \n",
    "    y_var_dict_partial = defaultdict()\n",
    "    y_p_var_dict_partial = defaultdict()\n",
    "    y_n_var_dict_partial = defaultdict()\n",
    "    \n",
    "    for quer in partial_dict.keys():\n",
    "        y_var_dict_partial[quer] = []\n",
    "        y_p_var_dict_partial[quer] = []\n",
    "        y_n_var_dict_partial[quer] = []\n",
    "        \n",
    "        positions_0 = []\n",
    "        positions_1 = []\n",
    "        for i, j in zip(quer[0],quer[1]):\n",
    "            if j == 0: \n",
    "                positions_0.append(i)\n",
    "            elif j ==1:\n",
    "                positions_1.append(i)\n",
    "        for i in range(n):\n",
    "            y_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_partial_{quer}_{i}\"))\n",
    "            y_p_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_p_partial_{quer}_{i}\"))\n",
    "            y_n_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_n_partial_{quer}_{i}\"))\n",
    "            \n",
    "            M.addConstr( len(quer[0])*y_var_dict_partial[quer][i] + (len(quer[0])+0.5)*y_p_var_dict_partial[quer][i] <= \n",
    "                        sum([x[i][j] for j in positions_1]) + sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( (len(quer[0])-0.5)*y_n_var_dict_partial[quer][i] + len(quer[0])*y_var_dict_partial[quer][i] + \n",
    "                        m*y_p_var_dict_partial[quer][i] >= \n",
    "                        sum([x[i][j] for j in positions_1])+ sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( y_var_dict_partial[quer][i] + y_p_var_dict_partial[quer][i] + y_n_var_dict_partial[quer][i] == 1)\n",
    "            \n",
    "        # this line does the work\n",
    "        M.addConstr(sum(y_var_dict_partial[quer]) >= partial_dict[quer])\n",
    "        \n",
    "    return None\n",
    "\n",
    "def bin_cons(M,x,n):\n",
    "    \"\"\" Adds binary symmetry breaking constraint \"\"\"\n",
    "    for i in range(n-1):\n",
    "        M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    return None\n",
    "\n",
    "def gen_bin_data_set(n,m):\n",
    "    \"\"\"n is number of people, m is number of attributes used, database is uniform random\"\"\"\n",
    "    db = pd.DataFrame(np.random.randint(0,2,size=(n, m)), columns=[f'att_{x}' for x in range(m)])\n",
    "    return db\n",
    "\n",
    "def gen_powerset(k):\n",
    "    \"\"\" generates a powerset of k elements \"\"\"\n",
    "    out = []\n",
    "    for i in itertools.product([0,1],repeat=k):\n",
    "        out.append(i)\n",
    "    return out\n",
    "\n",
    "def gen_queries_uniform_complete(m,n_queries):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return sample(queries,min(n_queries,len(queries)))\n",
    "\n",
    "def gen_queries_all(m):\n",
    "    \"\"\"Generates all possible queries of m binary attributes\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return queries\n",
    "\n",
    "def gen_queries_comp(m):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return queries\n",
    "\n",
    "def gen_queries_uniform(m,n_queries):\n",
    "    \"\"\"Generates random queries. Chooses number of attributes uniformly over [1,m],\n",
    "    Would be interested to see how this changes with a skewed distribution, also\n",
    "    would be interesting how it changes if symmetry in queries is forced\"\"\"\n",
    "    \n",
    "    queries = []\n",
    "\n",
    "    for i in range(n_queries):\n",
    "        quer = []\n",
    "        atts = np.random.randint(1,m+1)\n",
    "        quer.append(tuple(sorted(sample(range(m),atts))))\n",
    "        quer.append(tuple(np.random.choice([0,1], size=[atts])))\n",
    "        queries.append(quer)\n",
    "\n",
    "    queries = list(set([tuple(x) for x in queries]))\n",
    "\n",
    "    return queries\n",
    "\n",
    "def gen_partial_info(db,n_part,m_part):\n",
    "    \"\"\" generates a partial info dict based on n_partials rows\n",
    "    and m_partials attributes per individual, if no partial info, we get None type \"\"\"\n",
    "    \n",
    "    n = db.shape[0]\n",
    "    m = db.shape[1]\n",
    "    \n",
    "    partial_dict = defaultdict(lambda:0)\n",
    "    \n",
    "    if n_part == 0 or m_part == 0:\n",
    "        return partial_dict\n",
    "    \n",
    "    row_inds = np.random.choice(n,n_part, replace = False)\n",
    "    for i in row_inds:\n",
    "        att_sample = np.random.choice(m,m_part, replace = False)\n",
    "        lst = []\n",
    "        for j in att_sample:\n",
    "            lst.append(int(db[i:i+1][f\"att_{j}\"]))\n",
    "            \n",
    "        # print(att_sample,lst)\n",
    "        vals = [x for _,x in sorted(zip(att_sample,lst))]\n",
    "        \n",
    "        partial_dict[(tuple(sorted(att_sample)),tuple(vals))] += 1\n",
    "    return partial_dict\n",
    "            \n",
    "def quer2string(query):\n",
    "    \"\"\" converts query to a string to be used by pandas 'query' function\"\"\"\n",
    "    string = \"\"\n",
    "    for pos, att in enumerate(query[0]):\n",
    "        string += f'att_{att} == {query[1][pos]} & '\n",
    "    return string[0:-3]\n",
    "\n",
    "def get_counts_uniform(db,quers,noise):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + np.random.randint(-noise,noise+1),0)\n",
    "    return out_dict\n",
    "\n",
    "def binatodeci(binary):\n",
    "    return sum(val*(2**idx) for idx, val in enumerate(reversed(binary)))\n",
    "\n",
    "def check_fixed_sols(sols):\n",
    "    \"\"\"Takes some candidate databases and returns which\n",
    "    rows are contained in all of them, including duplicates\"\"\"\n",
    "    \n",
    "    if len(sols) == 1:\n",
    "        return [tuple(x) for x in sols[0]]\n",
    "    \n",
    "    out_lst = []\n",
    "    for sol in sols:\n",
    "        temp = []\n",
    "        for row in sol:\n",
    "            temp.append(tuple(row))\n",
    "        out_lst.append(temp)\n",
    "    # print(out_lst)\n",
    "\n",
    "    fixed_sols = []\n",
    "    for row in out_lst[0]:\n",
    "        count = 0\n",
    "        for sol in out_lst[1:]:\n",
    "            if row in sol:\n",
    "                sol.remove(row)\n",
    "                count += 1\n",
    "            # else:\n",
    "            #     break\n",
    "        if count == len(sols) - 1:\n",
    "            fixed_sols.append(row)\n",
    "    return fixed_sols\n",
    "\n",
    "def total_sim(sol,db):\n",
    "    count = 0\n",
    "    for i, val_i in enumerate(db):\n",
    "        for j, val_j in enumerate(val_i):\n",
    "            # print(val_j, sol[i][j])\n",
    "            if val_j != sol[i][j]:\n",
    "                count += 1\n",
    "            \n",
    "    return  1 - count/(len(db[0])*len(db))\n",
    "\n",
    "def gen_cij(db,sol,i,j):\n",
    "    \"\"\" computes cost of assigning row i of solution \n",
    "    to row j of database in terms of L1 norm\"\"\"\n",
    "    \n",
    "    cost = sum([abs(db[j][k]-sol[i][k]) for k in range(len(sol[0]))])\n",
    "    return cost\n",
    "\n",
    "def ass_ILP(db,sol):\n",
    "    \"\"\" Creates an optimal assignment of rows in the solution \n",
    "    to the rows in the true database, the objective function value\n",
    "    is returned and is the smallest number of differences between\n",
    "    solution and true database, based on row swapping\"\"\"\n",
    "    \n",
    "    n = len(sol)\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "               for j in range(n)] for i in range(n)])\n",
    "    \n",
    "    for i in range(n):\n",
    "        M.addConstr(sum(x[i,:]) == 1)\n",
    "        M.addConstr(sum(x[:,i]) == 1)\n",
    "        \n",
    "    M.setObjective(sum([sum([x[i][j]*gen_cij(db,sol,i,j) for j in range(n)]) for i in range(n)]), GRB.MINIMIZE)\n",
    "    \n",
    "    M.optimize()\n",
    "    \n",
    "    out_x = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            out_x[i][j] = x[i][j].Xn\n",
    "    # print(out_x)\n",
    "    \n",
    "    return 1 - M.ObjVal/(len(db)*len(db[0]))\n",
    "\n",
    "def get_counts_uniform_seed(db,quers,noise,quer_seed_dict):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + quer2noise_uniform(query,quer_seed_dict,noise),0)\n",
    "        # print(np.random.randint(-noise,noise+1))\n",
    "    return out_dict\n",
    "\n",
    "def get_counts_triangle_seed(db,quers,noise,quer_seed_dict):\n",
    "    \"\"\"Returns noisy count of query, noise is symmetric rounded triangle [-noise, +noise]\n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + quer2noise_triangle(query,quer_seed_dict,noise),0)\n",
    "        # print(np.random.randint(-noise,noise+1))\n",
    "    return out_dict\n",
    "\n",
    "def get_counts_triangle(db,quers,noise):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        if noise == 0:\n",
    "            actual_noise = 0\n",
    "        else:\n",
    "            actual_noise = round(np.random.triangular(-noise, 0, noise))\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + actual_noise,0)\n",
    "    return out_dict\n",
    "\n",
    "def quer2noise_uniform(query,quer_seed_dict,noise):\n",
    "    \"\"\" generates noise from discrete-RV: U({-noise,noise})\n",
    "    using the seed in the query dict\"\"\"\n",
    "    # print(query)\n",
    "    if quer_seed_dict != None:\n",
    "        np.random.seed(quer_seed_dict[query])\n",
    "    return np.random.randint(-noise,noise+1)\n",
    "\n",
    "def quer2noise_triangle(query,quer_seed_dict,noise):\n",
    "    \"\"\" generates noise from rounded triangle distribution over -noise to +noise, median at 0,\n",
    "    using the seed in the query dict\"\"\"\n",
    "    # print(query)\n",
    "    if quer_seed_dict != None:\n",
    "        np.random.seed(quer_seed_dict[query])\n",
    "    return round(np.random.triangular(-noise, 0, noise))\n",
    "\n",
    "def gen_powerset_test(k):\n",
    "    \"\"\" generates a powerset of k elements \"\"\"\n",
    "    out = []\n",
    "    for i in itertools.product([0,1],repeat=k):\n",
    "        out.append(i)\n",
    "    return out\n",
    "\n",
    "def flipped_choice(choice):\n",
    "    \"\"\" returns the compliment of a query\"\"\"\n",
    "    return tuple([abs(1-x) for x in choice])\n",
    "\n",
    "def gen_queries_compliment(m,n_queries):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            used = []\n",
    "            for choice in gen_powerset(i):\n",
    "                if choice not in used:\n",
    "                    quer = []\n",
    "                    used.append(flipped_choice(choice))\n",
    "                    quer.append(comb)\n",
    "                    quer.append(choice)\n",
    "                    queries.append(tuple(quer))\n",
    "            \n",
    "            \n",
    "    sampled = sample(queries,min(int(n_queries/2),len(queries)))\n",
    "    sample_with_compliment = list(sampled)\n",
    "    for quer in sampled:\n",
    "        sample_with_compliment.append((quer[0],flipped_choice(quer[1])))  \n",
    "    \n",
    "    return sample_with_compliment\n",
    "\n",
    "def query_seed_dict(m):\n",
    "    \"\"\" assigns a seed to every possible query based on the m attributes\"\"\"\n",
    "    quer_dict = defaultdict()\n",
    "    queries = gen_queries_uniform_complete(m,max_quers(m))\n",
    "    for query in queries:\n",
    "        quer_dict[query] = int(np.random.uniform()*100000)\n",
    "    # print(quer_dict)\n",
    "    return quer_dict\n",
    "\n",
    "def max_quers(m):\n",
    "    \"\"\"returns the maximum number of queries possible from m binary attributes\"\"\"\n",
    "    tot = 0\n",
    "    for i in range(1,m+1):\n",
    "        tot += (math.factorial(m)/(math.factorial(i)*math.factorial(m-i)))*(2**i)\n",
    "    return int(tot)\n",
    "\n",
    "def gen_query_row_pair(N_q_start,n,m,c,N_sols):\n",
    "    \"\"\" creates a database, and returns pairs of backbone solutions and the additional queries \n",
    "    (and their values) it took to find that backbone solution. Starts with N_q_start queries\n",
    "    and considers N_sols to create the backbone solutions\"\"\"\n",
    "    \n",
    "    max_quer = max_quers(m)\n",
    "    print(max_quer)\n",
    "    noise = c\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    display(db)\n",
    "    \n",
    "    # might not need to be shuffled, just making sure\n",
    "    quers = gen_queries_comp(m)\n",
    "    np.random.shuffle(quers)\n",
    "    quer_seed_dict = query_seed_dict(m)\n",
    "    quer_dict_all = get_counts_uniform_seed(db,quers,noise,quer_seed_dict)\n",
    "\n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    return_lst = []\n",
    "    backbones = []\n",
    "    fixed_sols =[]\n",
    "    quer_set = set()\n",
    "\n",
    "    N_q = min(int(N_q_start),max_quer)\n",
    "    dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n",
    "    \n",
    "    while (len(backbones) != n) and (N_q != max_quer+1):\n",
    "        \n",
    "        queries = quers[0:N_q]\n",
    "        quer_dict = dict((k, quer_dict_all[k]) for k in queries)\n",
    "        sols_all = gen_sols(n,m,quer_dict,noise,N_sols)\n",
    "        sols = np.array(sols_all[0], dtype=int) \n",
    "        \n",
    "        if len(sols) < N_sols:\n",
    "            fixed_sols = check_fixed_sols(sols)\n",
    "            \n",
    "        if len(fixed_sols) > len(backbones):\n",
    "\n",
    "            fs_c = Counter(fixed_sols)\n",
    "            b_c = Counter(backbones)\n",
    "            if sum(fs_c.values()) > sum(b_c.values()):\n",
    "                new_bones = [x for x in fs_c-b_c]\n",
    "            crit_query = {queries[N_q-1]: quer_dict[queries[N_q-1]]}\n",
    "            return_lst.append((new_bones,crit_query,dictfilt(quer_dict,set(queries).difference(quer_set))))\n",
    "            backbones += new_bones\n",
    "            quer_set.update(set(queries))\n",
    "        N_q += 1\n",
    "        \n",
    "    if N_q == max_quer -1:\n",
    "        print(\"Used all queries\")\n",
    "    return return_lst\n",
    "        \n",
    "def get_results_old(c,n,m,N_q,N_sols,dist,n_part,m_part):\n",
    "    \"\"\" Returns some performance metrics for a randomly generated database, with random noise\"\"\"   \n",
    "    noise = c\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    # print(db)\n",
    "    quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "    if n_part == 0 or m_part == 0:\n",
    "        partial_dict = None\n",
    "    else:\n",
    "        partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "    \n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform(db,quers,noise)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle(db,quers,noise)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "    \n",
    "    \n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    # print(db_tup)\n",
    "\n",
    "    #solving:\n",
    "    new_n = n \n",
    "    \n",
    "    sols_all = gen_sols(new_n,m,quer_dict,partial_dict,noise,N_sols)\n",
    "    sols = np.array(sols_all[0], dtype=int) \n",
    "    time = sols_all[1] \n",
    "    \n",
    "    backbone_size = 0\n",
    "    if len(sols) < N_sols:\n",
    "        fixed_sols = check_fixed_sols(sols)\n",
    "        backbone_size = len(fixed_sols)/n\n",
    "    \n",
    "    row_sims = []\n",
    "    tot_sims = []\n",
    "    counter_sols = []\n",
    "    \n",
    "    for sol in sols:\n",
    "        sol_tup = [tuple(x) for x in sol]\n",
    "        # print(sol_tup)\n",
    "        counter_sols.append(Counter(list(sol_tup)))\n",
    "        sol_tup, common = sol_tup[:], [ e for e in db_tup if e in sol_tup and (sol_tup.pop(sol_tup.index(e)) or True)]\n",
    "        # tot_sims.append(ass_ILP(db_tup,sol_tup))\n",
    "        row_sims.append(len(common)/n)\n",
    "        \n",
    "    crit_query_count = 0\n",
    "    query_items = quer_dict.items()\n",
    "    for query in gen_crit_queries(db_tup,m,c):\n",
    "        if query in query_items:\n",
    "            crit_query_count += 1\n",
    "    \n",
    "    returns = {\n",
    "        'maximum queries': max_quers(m),\n",
    "        'backbone_size': backbone_size, \n",
    "        # 'tot_avg': np.mean(tot_sims), \n",
    "        # 'tot_min': np.min(tot_sims),\n",
    "        'row_avg': np.mean(row_sims), \n",
    "        # 'row_min': np.min(row_sims), \n",
    "        'solve_time': time, \n",
    "        'N_sols_actual': len(sols),\n",
    "        \"Contained\": Counter(db_tup) in counter_sols,\n",
    "        \"crit_queries\": crit_query_count\n",
    "    }\n",
    "    # print(counter_sols)\n",
    "    return returns\n",
    "\n",
    "def get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict):\n",
    "    \"\"\" Returns some performance metrics for a randomly generated database, with random noise....\n",
    "    \n",
    "    db: is database as a pandas array\n",
    "    seed_dict: a dictionary assigning a seed to every possible query\n",
    "    quers: a bunch of queries in a two-tuple form, will get turned into a noisy count based on 'db', 'dist' and 'seed_dict'\n",
    "    c: bound on noise, integer\n",
    "    n: size of database (eventually want this to get rid of this...)\n",
    "    m: number of attributes\n",
    "    N_sols: number of solutions to enumerate when solving\n",
    "    dist: rn either 'uniform' or 'triangle', the distribution of the noise\n",
    "    partial_dict: dictionary containing partial information.\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    \n",
    "    noise = c\n",
    "#     # print(db)\n",
    "#     quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "#     if n_part == 0 or m_part == 0:\n",
    "#         partial_dict = None\n",
    "#     else:\n",
    "#         partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "\n",
    "\n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform_seed(db,quers,noise,quer_seed_dict)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle_seed(db,quers,noise,quer_seed_dict)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "\n",
    "    \n",
    "    \n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    # print(db_tup)\n",
    "\n",
    "    #solving:\n",
    "    new_n = n \n",
    "    \n",
    "    sols_all = gen_sols(new_n,m,quer_dict,partial_dict,noise,N_sols)\n",
    "    sols = np.array(sols_all[0], dtype=int) \n",
    "    time = sols_all[1] \n",
    "    \n",
    "    backbone_size = 0\n",
    "    if len(sols) < N_sols:\n",
    "        fixed_sols = check_fixed_sols(sols)\n",
    "        backbone_size = len(fixed_sols)/n\n",
    "    \n",
    "    row_sims = []\n",
    "    tot_sims = []\n",
    "    counter_sols = []\n",
    "    \n",
    "    \n",
    "    sol_set = set()\n",
    "    for sol in sols:\n",
    "        sol_tup = [tuple(x) for x in sol]\n",
    "        sol_set.add(tuple(sol_tup))\n",
    "        # print(sol_tup)\n",
    "        counter_sols.append(Counter(list(sol_tup)))\n",
    "        sol_tup, common = sol_tup[:], [ e for e in db_tup if e in sol_tup and (sol_tup.pop(sol_tup.index(e)) or True)]\n",
    "        # tot_sims.append(ass_ILP(db_tup,sol_tup))\n",
    "        row_sims.append(len(common)/n)\n",
    "        \n",
    "    crit_query_count = 0\n",
    "    query_items = quer_dict.items()\n",
    "    for query in gen_crit_queries(db_tup,m,c):\n",
    "        if query in query_items:\n",
    "            crit_query_count += 1\n",
    "            \n",
    "    \n",
    "    # print(len(sol_set))\n",
    "    # print(sols)\n",
    "    returns = {\n",
    "        'maximum queries': max_quers(m),\n",
    "        'backbone_size': backbone_size, \n",
    "        # 'tot_avg': np.mean(tot_sims), \n",
    "        # 'tot_min': np.min(tot_sims),\n",
    "        'row_avg': np.mean(row_sims), \n",
    "        # 'row_min': np.min(row_sims), \n",
    "        'solve_time': time, \n",
    "        'N_sols_actual': len(sols),\n",
    "        \"Contained\": Counter(db_tup) in counter_sols,\n",
    "        \"crit_queries\": crit_query_count\n",
    "    }\n",
    "    # print(counter_sols)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def gen_crit_queries(db,m,c):\n",
    "    outputs = []\n",
    "    A = tuple(range(m))\n",
    "    for row in db:\n",
    "        V = tuple(row)\n",
    "        for i in range(3):\n",
    "            outputs.append(((A,V),c+i))\n",
    "    return outputs\n",
    "\n",
    "def compare_noise_types(n,m,c,N_q,N_sols,N_trials,n_part,m_part):\n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    tri_list = []\n",
    "    uni_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    quer_seed_dict = None\n",
    "    \n",
    "    for i in range(N_trials):\n",
    "        for dist in ['triangle','uniform']:\n",
    "            quers = gen_queries_uniform_complete(m,N_q)\n",
    "            db = gen_bin_data_set(n,m)\n",
    "            partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "            \n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist)\n",
    "            results[dist].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[dist] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for dist in ['triangle','uniform']:\n",
    "        df = pd.DataFrame(results[dist])\n",
    "        out_results[dist] = dict(df.mean()) \n",
    "        out_results[dist]['frac_solved_uniqely'] = unique_counts[dist]/N_trials\n",
    "        out_results[dist]['params'] = {'n': n, 'm': m, 'c': c, 'N_q': N_q, 'N_sols': N_sols, 'N_trials': N_trials}\n",
    "    return out_results\n",
    "\n",
    "def gen_queries_count_size(db,c,dist,quer_seed_dict,N_q,first):\n",
    "    quers_all = gen_queries_uniform_complete(m,max_quers(m))\n",
    "    if dist == 'uniform': quer_counts = get_counts_uniform_seed(db,quers_all,c,quer_seed_dict)\n",
    "    if dist == 'triangle': quer_counts = get_counts_triangle_seed(db,quers_all,c,quer_seed_dict)\n",
    "    \n",
    "    if first == 'large':\n",
    "        sorted_quers = {k: v for k, v in sorted(quer_counts.items(), key=lambda item: item[1], reverse = True)}\n",
    "    elif first == 'small':\n",
    "        sorted_quers = {k: v for k, v in sorted(quer_counts.items(), key=lambda item: item[1])}\n",
    "        \n",
    "    first_Nq_quers = {k: sorted_quers[k] for k in list(sorted_quers.keys())[0:N_q]}\n",
    "    # print(first_Nq_quers)\n",
    "    return list(first_Nq_quers.keys())\n",
    "\n",
    "def gen_queries(m,n,db,c,quer_seed_dict,N_q,quer_type,dist):\n",
    "    \n",
    "    N_q = min(N_q,max_quers(m))\n",
    "    all_quers = gen_queries_all(m)\n",
    "    \n",
    "    if quer_type == 'uniform_random':\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "    elif quer_type == 'large_counts_first':\n",
    "        quers = gen_queries_count_size(db,c,dist,quer_seed_dict,N_q,'large')\n",
    "        \n",
    "    elif quer_type == 'small_counts_first':\n",
    "        quers = gen_queries_count_size(db,c,dist,quer_seed_dict,N_q,'small')\n",
    "        \n",
    "    elif quer_type == 'many_atts_first':\n",
    "        quers = sorted(all_quers,key =lambda item: len(item[0]), reverse = True)[0:N_q]\n",
    "    \n",
    "    elif quer_type == 'few_atts_first':\n",
    "        quers = sorted(all_quers,key =lambda item: len(item[0]))[0:N_q]\n",
    "        \n",
    "    return quers\n",
    "        \n",
    "    \n",
    "\n",
    "def compare_query_types(n,m,c,N_q,N_sols,N_trials,dist,quer_types):\n",
    "    \n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    normal_list = []\n",
    "    partial_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        # quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        for quer_type in quer_types:\n",
    "            # partial_dict = gen_partial_info(db,partial[0],partial[1])\n",
    "            # print(partial_dict)\n",
    "            \n",
    "            partial_dict = dict()\n",
    "            quers = gen_queries(m,n,db,c,quer_seed_dict,N_q,quer_type,dist)\n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist,partial[0],partial[1])\n",
    "            results[quer_type].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[quer_type] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for quer_type in quer_types:\n",
    "        df = pd.DataFrame(results[quer_type])\n",
    "        out_results[quer_type] = dict(df.mean()) \n",
    "        out_results[quer_type]['frac_solved_uniqely'] = unique_counts[quer_type]/N_trials\n",
    "        out_results[quer_type]['params'] = {'n': int(n), 'm': int(m), 'c': int(c), 'N_q': int(N_q), \n",
    "                                               'N_sols': int(N_sols), 'N_trials': int(N_trials)}\n",
    "        # out_results[]\n",
    "        \n",
    "    return out_results\n",
    "\n",
    "def compare_partial_info(n,m,c,N_q,N_sols,N_trials,dist,partials):\n",
    "    \"\"\"partials is a list of 2-element lists, where the first position is n_part,\n",
    "    the number of people to generate partial info from, and second is m_part, \n",
    "    the number of attributes for each person to 'know'. Comapres each 2-element list, \n",
    "    asking N_q queries on each, averaging N_trials times\n",
    "    \n",
    "    should this all be done with additive query style? \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    normal_list = []\n",
    "    partial_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        for partial in partials:\n",
    "            partial_dict = gen_partial_info(db,partial[0],partial[1])\n",
    "            # print(partial_dict)\n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist,partial[0],partial[1])\n",
    "            results[str(partial)].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[str(partial)] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for partial in partials:\n",
    "        df = pd.DataFrame(results[str(partial)])\n",
    "        out_results[str(partial)] = dict(df.mean()) \n",
    "        out_results[str(partial)]['frac_solved_uniqely'] = unique_counts[tuple(partial)]/N_trials\n",
    "        out_results[str(partial)]['params'] = {'n': int(n), 'm': int(m), 'c': int(c), 'N_q': int(N_q), \n",
    "                                               'N_sols': int(N_sols), 'N_trials': int(N_trials)}\n",
    "        # out_results[]\n",
    "    return out_results\n",
    "\n",
    "def single_basic_run(n,m,c,N_q,N_sols,N_trials,dist,n_part,m_part):\n",
    "    \"\"\" Generates a dataset and solves, averaging over N_trials times.\n",
    "    returns a dictionary with all the performance 'metrics'\"\"\"\n",
    "    \n",
    "    out_lst = []\n",
    "    unique_counts = 0\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        # print(db)\n",
    "        partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "        # print(partial_dict)\n",
    "        temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "        out_lst.append(temp_res)\n",
    "        if temp_res['N_sols_actual'] == 1:\n",
    "            unique_counts += 1\n",
    "            \n",
    "    df = pd.DataFrame(out_lst)\n",
    "    out_dict = dict(df.mean()) \n",
    "    out_dict['frac_solved_uniqely'] = unique_counts/N_trials\n",
    "\n",
    "    return out_dict\n",
    "    \n",
    "def compare_query_types_additive(n,m,c,N_q_range,N_sols,N_trials,dist,quer_types):\n",
    "    \n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict()\n",
    "    out_lst = []\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        quers_all = defaultdict()\n",
    "        for quer_type in quer_types:\n",
    "            quers_all[quer_type] = gen_queries(m,n,db,c,quer_seed_dict,max_quers(m),quer_type,dist)\n",
    "            \n",
    "        partial_dict = dict()\n",
    "        for N_q_try in N_q_range:\n",
    "            N_q = int(min(N_q_try,max_quers(m)))\n",
    "\n",
    "            temp_dict = defaultdict()\n",
    "            \n",
    "            for quer_type in quer_types:\n",
    "                \n",
    "                quers = quers_all[quer_type][0:N_q]\n",
    "                temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "                temp_dict[quer_type] = temp_res\n",
    "                \n",
    "            results[N_q] = temp_dict\n",
    "            \n",
    "        out_lst.append(results)\n",
    "    return ave(out_lst)\n",
    "\n",
    "def ave(d):\n",
    "    \"\"\"given a list of nested dictionaries, it will return a dictionary of averages \n",
    "    in the same format as the individual dicts\"\"\"\n",
    "    _data = sorted([i for b in d for i in b.items()], key=lambda x:x[0])\n",
    "    _d = [(a, [j for _, j in b]) for a, b in groupby(_data, key=lambda x:x[0])]\n",
    "    return {a:ave(b) if isinstance(b[0], dict) else sum(b)/float(len(b)) for a, b in _d}\n",
    "\n",
    "def get_quer_counts(db,m,c,quer_seed_dict,dist):\n",
    "    quers = quer_seed_dict.keys()\n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform_seed(db,quers,c,quer_seed_dict)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle_seed(db,quers,c,quer_seed_dict)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "    return quer_dict\n",
    "\n",
    "# def save_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d4ff8-e9cc-48f0-a9b5-2e58586b9404",
   "metadata": {},
   "source": [
    "## Making data for query type comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f8f1325-4b8e-42bf-91c8-ca0043c1d1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = 2\n",
    "n = 10\n",
    "m = 5\n",
    "# N_q = 150\n",
    "N_sols = 1000\n",
    "N_trials = 20\n",
    "dist = 'uniform'\n",
    "db = gen_bin_data_set(n,m)\n",
    "quer_seed_dict = query_seed_dict(m)\n",
    "\n",
    "# quer_type = 'uniform_random'\n",
    "# quer_type = 'large_counts_first'\n",
    "# quer_type = 'small_counts_first'\n",
    "# quer_type = 'many_atts_first'\n",
    "quer_types = ['uniform_random','large_counts_first','small_counts_first','many_atts_first','few_atts_first']\n",
    "\n",
    "\n",
    "compare_query_types(n,m,c,N_q,N_sols,N_trials,dist,quer_types)\n",
    "\n",
    "params = f\"c_{c}_n_{n}_m_{m}\"\n",
    "\n",
    "data = defaultdict()\n",
    "for N_q in np.linspace(70,250,13,dtype = int):\n",
    "    data[int(N_q)] = compare_query_types(n,m,c,N_q,N_sols,N_trials,dist,quer_types)\n",
    "\n",
    "filename = f'compare_quer_type/{params}_{quer_types}.json'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f: \n",
    "    # json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a85c66ce-d798-40a7-846e-2b7e80cafcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hmcarthur/differential_privacy/toy_model_ILP'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18198c5-ff25-4e98-a4c8-6de6219955b0",
   "metadata": {},
   "source": [
    "## produce data for additive query style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4824d0d-e537-4f8d-b8cf-9af6f604392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2\n",
    "n = 10\n",
    "m = 5\n",
    "N_q_range = np.linspace(10,50,15,dtype = int)\n",
    "N_sols = 2000\n",
    "N_trials = 20\n",
    "# quer_types = ['uniform_random','large_counts_first','small_counts_first','many_atts_first','few_atts_first']\n",
    "quer_types = ['large_counts_first']\n",
    "data = compare_query_types_additive(n,m,c,N_q_range,N_sols,N_trials,dist,quer_types)\n",
    "params = f\"c_{c}_n_{n}_m_{m}_large_counts_first\"\n",
    "\n",
    "filename = f'compare_quer_type_additive/{params}.json'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f: \n",
    "    json.dump(data, f)\n",
    "\n",
    "filename_params = f'compare_quer_type_additive/{params}_params.json'\n",
    "param_dict = {'n': int(n), 'm': int(m), 'c': int(c), 'N_q_range': [int(x) for x in N_q_range], \n",
    "                                               'N_sols': int(N_sols), 'N_trials': int(N_trials)}\n",
    "os.makedirs(os.path.dirname(filename_params), exist_ok=True)\n",
    "with open(filename_params, 'w') as f: \n",
    "    json.dump(param_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ee0c336-e7e0-4625-ba92-2b529b01420e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.compare_noise_types.<locals>.<lambda>()>,\n",
       "            {'triangle': {'maximum queries': 728.0,\n",
       "              'backbone_size': 1.0,\n",
       "              'row_avg': 1.0,\n",
       "              'solve_time': 0.2295980453491211,\n",
       "              'N_sols_actual': 1.0,\n",
       "              'Contained': 1.0,\n",
       "              'crit_queries': 5.0,\n",
       "              'frac_solved_uniqely': 1.0,\n",
       "              'params': {'n': 5,\n",
       "               'm': 6,\n",
       "               'c': 1,\n",
       "               'N_q': 728,\n",
       "               'N_sols': 10000,\n",
       "               'N_trials': 1}},\n",
       "             'uniform': {'maximum queries': 728.0,\n",
       "              'backbone_size': 1.0,\n",
       "              'row_avg': 1.0,\n",
       "              'solve_time': 0.17855501174926758,\n",
       "              'N_sols_actual': 1.0,\n",
       "              'Contained': 1.0,\n",
       "              'crit_queries': 2.0,\n",
       "              'frac_solved_uniqely': 1.0,\n",
       "              'params': {'n': 5,\n",
       "               'm': 6,\n",
       "               'c': 1,\n",
       "               'N_q': 728,\n",
       "               'N_sols': 10000,\n",
       "               'N_trials': 1}}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 1\n",
    "n = 5\n",
    "m = 6\n",
    "N_q = 728\n",
    "N_sols = 10000\n",
    "N_trials = 1\n",
    "compare_noise_types(n,m,c,N_q,N_sols,N_trials,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70401c61-0a8c-40bc-b6d9-a11f170e488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2\n",
    "n = 3\n",
    "m = 10\n",
    "N_q = 5330\n",
    "N_sols = 3\n",
    "n_part = 2\n",
    "m_part = 7\n",
    "\n",
    "display(get_results(c,n,m,N_q,N_sols,'uniform',n_part,m_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf41bb9c-5093-44f7-9c70-0e350dd1ec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'maximum queries': 80.0,\n",
       " 'backbone_size': 0.4439999999999999,\n",
       " 'row_avg': 0.8045953272270779,\n",
       " 'solve_time': 0.025852453708648682,\n",
       " 'N_sols_actual': 48.02,\n",
       " 'Contained': 1.0,\n",
       " 'crit_queries': 8.97,\n",
       " 'frac_solved_uniqely': 0.01}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = 1\n",
    "n = 10\n",
    "m = 4\n",
    "N_q = 142\n",
    "N_sols = 1000\n",
    "N_trials = 100\n",
    "n_part = 10\n",
    "m_part = 0\n",
    "dist = 'triangle'\n",
    "\n",
    "display(single_basic_run(n,m,c,N_q,N_sols,N_trials,dist,n_part,m_part))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9ef45-d0a1-43bb-9cad-f1724340ece7",
   "metadata": {},
   "source": [
    "## Making some data to plot a comparison of partial info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2729d9c5-a72e-4b11-9fa6-6fcf2fa483ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2\n",
    "n = 10\n",
    "m = 5\n",
    "# N_q = 150\n",
    "N_sols = 1000\n",
    "N_trials = 15\n",
    "partials = [[0,0],[2,4],[4,2],[4,4],[6,4],[10,2]]\n",
    "dist = 'uniform'\n",
    "params = f\"c_{c}_n_{n}_m_{m}\"\n",
    "\n",
    "data = defaultdict()\n",
    "for N_q in np.linspace(100,250,10,dtype = int):\n",
    "    data[int(N_q)] = compare_partial_info(n,m,c,N_q,N_sols,N_trials,dist,partials)\n",
    "\n",
    "filename = f'compare_partials_data/{params}_{partials}.json'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f: \n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23d5bec-f068-4a20-8df4-07e7f2626b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_count(dist,quer_seed_dict,db,quer,c):\n",
    "    quers = [quer]\n",
    "    if dist == 'uniform':\n",
    "            quer_dict = get_counts_uniform_seed(db,quers,c,quer_seed_dict)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle_seed(db,quers,c,quer_seed_dict)\n",
    "    return list(quer_dict.values())[0]\n",
    "\n",
    "def sort_quer(quer):\n",
    "    vals = [x for _,x in sorted(zip(quer[0],quer[1]))]\n",
    "    return (tuple(sorted(quer[0])),tuple(vals))\n",
    "\n",
    "def gen_queries_algo(db,m,quer_seed_dict,dist,c,N_q):\n",
    "    \n",
    "    quers = defaultdict()\n",
    "    seeds = np.random.randint(0,100000,1000)\n",
    "    i=0\n",
    "    \n",
    "    while len(quers.keys()) < min(max_quers(m),N_q):\n",
    "        \n",
    "        np.random.seed(seeds[i])\n",
    "        order = list(np.random.choice(list(range(m)),m, replace = False))\n",
    "        # print(order)\n",
    "        A = []\n",
    "        V = []\n",
    "        for i in order:\n",
    "            A.append(i)\n",
    "            quer0 = sort_quer([A,V+[0]])\n",
    "            quer1 = sort_quer([A,V+[1]])\n",
    "            \n",
    "            val0 = produce_count(dist,quer_seed_dict,db,quer0,c)\n",
    "            val1 = produce_count(dist,quer_seed_dict,db,quer1,c)\n",
    "            \n",
    "            # print(val0)\n",
    "            \n",
    "            if quer0 in quers.keys():\n",
    "                V+= [1]\n",
    "            elif quer1 in quers.keys():\n",
    "                V+= [0]\n",
    "            else:\n",
    "                V += [np.argmax([val0,val1])]\n",
    "            quers[quer0] = val0\n",
    "            quers[quer1] = val1\n",
    "            # print(A,V)\n",
    "        i+=1\n",
    "        \n",
    "    return quers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6120d96-5da3-4dbb-9d8e-b23a92032c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 4, 2, 0, 1]\n",
      "[5] [0]\n",
      "[5, 3] [0, 0]\n",
      "[5, 3, 4] [0, 0, 0]\n",
      "[5, 3, 4, 2] [0, 0, 0, 0]\n",
      "[5, 3, 4, 2, 0] [0, 0, 0, 0, 1]\n",
      "[5, 3, 4, 2, 0, 1] [0, 0, 0, 0, 1, 1]\n",
      "[1, 4, 2, 3, 5, 0]\n",
      "[1] [1]\n",
      "[1, 4] [1, 1]\n",
      "[1, 4, 2] [1, 1, 0]\n",
      "[1, 4, 2, 3] [1, 1, 0, 0]\n",
      "[1, 4, 2, 3, 5] [1, 1, 0, 0, 1]\n",
      "[1, 4, 2, 3, 5, 0] [1, 1, 0, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {((5,), (0,)): 51,\n",
       "             ((5,), (1,)): 47,\n",
       "             ((3, 5), (0, 0)): 28,\n",
       "             ((3, 5), (1, 0)): 25,\n",
       "             ((3, 4, 5), (0, 0, 0)): 14,\n",
       "             ((3, 4, 5), (0, 1, 0)): 12,\n",
       "             ((2, 3, 4, 5), (0, 0, 0, 0)): 10,\n",
       "             ((2, 3, 4, 5), (1, 0, 0, 0)): 6,\n",
       "             ((0, 2, 3, 4, 5), (0, 0, 0, 0, 0)): 2,\n",
       "             ((0, 2, 3, 4, 5), (1, 0, 0, 0, 0)): 7,\n",
       "             ((0, 1, 2, 3, 4, 5), (1, 0, 0, 0, 0, 0)): 2,\n",
       "             ((0, 1, 2, 3, 4, 5), (1, 1, 0, 0, 0, 0)): 4,\n",
       "             ((1,), (0,)): 44,\n",
       "             ((1,), (1,)): 55,\n",
       "             ((1, 4), (1, 0)): 24,\n",
       "             ((1, 4), (1, 1)): 31,\n",
       "             ((1, 2, 4), (1, 0, 1)): 18,\n",
       "             ((1, 2, 4), (1, 1, 1)): 13,\n",
       "             ((1, 2, 3, 4), (1, 0, 0, 1)): 10,\n",
       "             ((1, 2, 3, 4), (1, 0, 1, 1)): 8,\n",
       "             ((1, 2, 3, 4, 5), (1, 0, 0, 1, 0)): 3,\n",
       "             ((1, 2, 3, 4, 5), (1, 0, 0, 1, 1)): 5,\n",
       "             ((0, 1, 2, 3, 4, 5), (0, 1, 0, 0, 1, 1)): 5,\n",
       "             ((0, 1, 2, 3, 4, 5), (1, 1, 0, 0, 1, 1)): 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n =100\n",
    "m = 6\n",
    "c = 1\n",
    "N_q =20\n",
    "db = gen_bin_data_set(n,m)\n",
    "quer_seed_dict = query_seed_dict(m)\n",
    "dist = 'uniform'\n",
    "gen_queries_algo(db,m,quer_seed_dict,dist,c,N_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b483070-bfb1-415a-ac60-39e3c70d2ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_quers(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "11acb903-a141-44e7-b3b0-ad55ddaebd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boo():\n",
    "    for i in range(10):\n",
    "        print(np.random.randint(10))\n",
    "        par()\n",
    "    return None\n",
    "    \n",
    "def par():\n",
    "    np.random.seed(10)\n",
    "    # print(np.random.randint(10))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d73ace7-4e51-469d-88c1-80f51a295287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_edges(quer_tails,quer_heads,numbering):\n",
    "    edge_lst = []\n",
    "    for tail in quer_tails:\n",
    "        for head in quer_heads:\n",
    "            if set(split_quer(tail)).issubset(set(split_quer(head))):\n",
    "                edge_lst.append([numbering[tail],numbering[head]])\n",
    "    return edge_lst\n",
    "            \n",
    "def split_quer(quer):\n",
    "    lst = []\n",
    "    for i,j in zip(quer[0],quer[1]):\n",
    "        lst.append((i,j))\n",
    "    return lst\n",
    "\n",
    "def get_edges(numbering,m):\n",
    "    quer_sizes = defaultdict(list)\n",
    "    for query in numbering.keys():\n",
    "        quer_sizes[len(query[0])].append(query)\n",
    "    edges = []\n",
    "    for i in range(0,m):\n",
    "        edges += identify_edges(quer_sizes[i],quer_sizes[i+1],numbering)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "c5d4e328-af69-42d3-bf7c-76c8f9c2af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"nodes_length_hierarchy.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10d079220>"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 3\n",
    "numbering = defaultdict()\n",
    "i = 0 \n",
    "queries = gen_queries_all(m)\n",
    "queries.append(((),()))\n",
    "for quer in queries:\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "edges = get_edges(numbering,m)\n",
    "\n",
    "net = Network(notebook = True, layout = True, height='500px', width='100%')\n",
    "# net.add_nodes(list(numbering.values()), label = [str(x) for x in numbering.keys()], level = [0 for x in numbering.keys()])\n",
    "for query in queries:\n",
    "    net.add_node(numbering[query], str(query), level = len(query[0]))\n",
    "\n",
    "net.add_edges(edges)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# display(HTML('nodes.html'))\n",
    "net.show('nodes_length_hierarchy.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8876c84-7b3e-4e8c-b6ae-40399367b09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_0</th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   att_0  att_1  att_2\n",
       "0      1      0      1\n",
       "1      0      0      1\n",
       "2      1      1      1\n",
       "3      0      0      0\n",
       "4      1      1      0\n",
       "5      0      0      1\n",
       "6      0      1      0\n",
       "7      1      0      1\n",
       "8      0      0      1\n",
       "9      1      0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {((0, 1, 2), (1, 0, 0)): 1,\n",
       "             ((2,), (1,)): 6,\n",
       "             ((0, 2), (0, 1)): 2,\n",
       "             ((0, 1, 2), (0, 0, 1)): 4,\n",
       "             ((0, 2), (0, 0)): 3,\n",
       "             ((0, 1, 2), (1, 1, 0)): 2,\n",
       "             ((0,), (1,)): 4,\n",
       "             ((0, 2), (1, 1)): 5,\n",
       "             ((0, 1), (0, 1)): 1,\n",
       "             ((1, 2), (1, 1)): 0,\n",
       "             ((0, 1), (1, 1)): 2,\n",
       "             ((2,), (0,)): 2,\n",
       "             ((1,), (1,)): 2,\n",
       "             ((1,), (0,)): 6,\n",
       "             ((1, 2), (0, 1)): 5,\n",
       "             ((1, 2), (0, 0)): 2,\n",
       "             ((0, 1, 2), (0, 1, 1)): 0,\n",
       "             ((0,), (0,)): 6,\n",
       "             ((0, 1, 2), (0, 1, 0)): 2,\n",
       "             ((0, 1, 2), (1, 0, 1)): 4,\n",
       "             ((0, 1, 2), (0, 0, 0)): 0,\n",
       "             ((0, 1, 2), (1, 1, 1)): 2,\n",
       "             ((1, 2), (1, 0)): 1,\n",
       "             ((0, 1), (0, 0)): 3,\n",
       "             ((0, 1), (1, 0)): 4,\n",
       "             ((0, 2), (1, 0)): 0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-09-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bidict({0: ((0,), (0,)), 1: ((0,), (1,)), 2: ((1,), (0,)), 3: ((1,), (1,)), 4: ((2,), (0,)), 5: ((2,), (1,)), 6: ((0, 1), (0, 0)), 7: ((0, 1), (0, 1)), 8: ((0, 1), (1, 0)), 9: ((0, 1), (1, 1)), 10: ((0, 2), (0, 0)), 11: ((0, 2), (0, 1)), 12: ((0, 2), (1, 0)), 13: ((0, 2), (1, 1)), 14: ((1, 2), (0, 0)), 15: ((1, 2), (0, 1)), 16: ((1, 2), (1, 0)), 17: ((1, 2), (1, 1)), 18: ((0, 1, 2), (0, 0, 0)), 19: ((0, 1, 2), (0, 0, 1)), 20: ((0, 1, 2), (0, 1, 0)), 21: ((0, 1, 2), (0, 1, 1)), 22: ((0, 1, 2), (1, 0, 0)), 23: ((0, 1, 2), (1, 0, 1)), 24: ((0, 1, 2), (1, 1, 0)), 25: ((0, 1, 2), (1, 1, 1))})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[19, 18.568238213399503, 4],\n",
       " [24, 21.328125, 2],\n",
       " [17, 25.54963680387409, 0],\n",
       " [4, 26.1294964028777, 2],\n",
       " [23, 28.804761904761904, 4],\n",
       " [25, 29.11948051948052, 2],\n",
       " [12, 30.38443396226415, 0],\n",
       " [3, 30.85496183206107, 2],\n",
       " [0, 30.992518703241895, 6],\n",
       " [13, 31.47710843373494, 5],\n",
       " [1, 31.496385542168674, 4],\n",
       " [18, 31.587064676616915, 0],\n",
       " [11, 31.81021897810219, 2],\n",
       " [20, 32.957040572792366, 2],\n",
       " [6, 34.25735294117647, 3],\n",
       " [15, 34.89208633093525, 5],\n",
       " [2, 34.981627296587924, 6],\n",
       " [10, 35.25257731958763, 3],\n",
       " [16, 35.51105651105651, 1],\n",
       " [14, 35.67821782178218, 2],\n",
       " [8, 35.83, 4],\n",
       " [5, 36.80861244019139, 6],\n",
       " [7, 37.0, 1],\n",
       " [21, 37.74, 0],\n",
       " [9, 38.53365384615385, 2],\n",
       " [22, 42.99197860962567, 1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def produce_paths(db,quer_seed_dict,n,m,c,dist,N_sols,k,N_paths,numbering):\n",
    "    \"\"\"k is path length\"\"\"\n",
    "    \n",
    "    partial_dict = dict()\n",
    "    all_quers = list(quer_seed_dict.keys()) \n",
    "\n",
    "    paths = []\n",
    "    sols_path_dict = defaultdict()\n",
    "    backbone_path_dict = defaultdict()\n",
    "    for i in range(N_paths):\n",
    "        path = tuple(np.random.choice(list(numbering.inverse.keys()),min(k,max_quers(m)), replace = False))\n",
    "        paths.append(path)\n",
    "    for path in paths:\n",
    "        quers = [numbering.inverse[x] for x in path]\n",
    "        res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "        sols_path_dict[path] = res['N_sols_actual']\n",
    "        backbone_path_dict[path] = res['backbone_size']\n",
    "        \n",
    "    return sols_path_dict, backbone_path_dict\n",
    "    \n",
    "def good_paths(path_dicts, numbering, sol_thresh, bb_thresh):\n",
    "    \"\"\"a path is a good path if it has leq sol_thresh number of feasible solutions, \n",
    "    or if it has geq bb_thresh backbone fraction.\"\"\"\n",
    "    \n",
    "    good_paths = []\n",
    "    good_quer_count = Counter()\n",
    "    quer_count = Counter()\n",
    "\n",
    "    for path in path_dicts[0].keys():\n",
    "        quer_count.update(path)\n",
    "        if path_dicts[0][path] <= sol_thresh or path_dicts[1][path] >= bb_thresh:\n",
    "            good_paths.append(path)\n",
    "            good_quer_count.update(path)\n",
    "    return good_paths, good_quer_count\n",
    "\n",
    "def quer_perf_avg(path_dicts,numbering):\n",
    "\n",
    "    quer_score_dict = defaultdict(lambda: 0)\n",
    "    quer_count = Counter()\n",
    "    # print(path_dicts)\n",
    "    \n",
    "    for path in path_dicts[0].keys():\n",
    "        quer_count.update(path)\n",
    "        for quer in path:\n",
    "            # print(quer)\n",
    "            quer_score_dict[quer] += path_dicts[0][path]\n",
    "    for quer in quer_score_dict.keys():\n",
    "        quer_score_dict[quer] /= quer_count[quer]\n",
    "    return quer_score_dict\n",
    "\n",
    "def get_n_sols(n,m,c,quers,quer_counts,N_sols):\n",
    "    \"\"\"returns number of feasible there are for choice of queries, where\n",
    "    info about database + noisy counts is given in quer_counts\"\"\"\n",
    "    quer_dict = defaultdict()\n",
    "    for quer in quers:\n",
    "        quer_dict[quer] = quer_counts[quer]\n",
    "    return n_sols(n,m,quer_dict,c,N_sols)\n",
    "\n",
    "n = 10\n",
    "m = 3\n",
    "k = 10\n",
    "c = 1\n",
    "N_paths = 1050\n",
    "N_sols = 1000\n",
    "dist = 'uniform'\n",
    "sol_thresh = 20\n",
    "bb_thresh = 0.7\n",
    "db = gen_bin_data_set(n,m)\n",
    "quer_seed_dict = query_seed_dict(m)\n",
    "display(db)\n",
    "quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "display(quer_counts)\n",
    "\n",
    "#create numbering, doing it outside of function if need to translate numbering to network graph\n",
    "numbering = bidict()\n",
    "i = 0 \n",
    "for quer in gen_queries_all(m):\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "\n",
    "path_dicts = produce_paths(db,quer_seed_dict,n,m,c,dist,N_sols,k,N_paths,numbering)\n",
    "\n",
    "\n",
    "display(numbering.inverse)\n",
    "\n",
    "sol_avg_dict = quer_perf_avg(path_dicts,numbering)\n",
    "# bb = list(sorted(sol_avg_dict.items(), key=lambda x:x[1]))\n",
    "new_lst = []\n",
    "for i in list(sorted(sol_avg_dict.items(), key=lambda x:x[1])):\n",
    "    a = list(i)\n",
    "    a.append(quer_counts[numbering.inverse[i[0]]])\n",
    "    new_lst.append(a)\n",
    "display(new_lst)\n",
    "\n",
    "# good_paths_out = good_paths(path_dicts, numbering, sol_thresh, bb_thresh)[1]\n",
    "# display(sorted(good_paths_out.items(), key=lambda x:x[1], reverse = True))\n",
    "\n",
    "selection = []\n",
    "for quer in new_lst[0:k]:\n",
    "    selection.append(numbering.inverse[quer[0]])\n",
    "print(get_n_sols(n,m,c,selection,quer_counts,N_sols))\n",
    "selection_big_counts = gen_queries(m,n,db,c,quer_seed_dict,k,'large_counts_first',dist)\n",
    "print(get_n_sols(n,m,c,selection_big_counts,quer_counts,N_sols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3a4f3a-f9c3-4eb0-9c13-c954f1312781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algo(n_iters, pop_size,n_parents, n_crossovers, n_mutants, quer_counts,n,m,c,N_sols,k,numbering):\n",
    "    largest_number = max_quers(m)\n",
    "    initial_pop = [np.random.choice(list(range(largest_number)), k, replace = False) for x in range(pop_size)]\n",
    "    fit_dict = defaultdict()\n",
    "    for i in initial_pop:\n",
    "        fit_dict[tuple(i)] = get_fitness(i,n,m,c,quer_counts,N_sols,numbering)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        if len(set(fit_dict.keys())) <= 3:\n",
    "            return list(set(fit_dict.keys()))[0], fit_dict[list(set(fit_dict.keys()))[0]]\n",
    "        parents = choose_parents(fit_dict,n_parents)\n",
    "        children = breed(parents,n_crossovers,largest_number)\n",
    "        pop = parents+children\n",
    "        pop += mutate(pop,n_mutants,largest_number)\n",
    "        new_fit_dict = cull(pop,fit_dict,pop_size,n,m,c,quer_counts,N_sols,numbering)\n",
    "        fit_dict = dict(new_fit_dict)\n",
    "        \n",
    "        ####this is a shitty fix - change this\n",
    "        if len(set(fit_dict.values())) == 1:\n",
    "            break\n",
    "    \n",
    "    min_fit = min(fit_dict.values())\n",
    "    for i in fit_dict.keys():\n",
    "        if fit_dict[i] == min_fit:\n",
    "            return sorted(i), fit_dict[i]\n",
    "    \n",
    "def get_fitness(sol,n,m,c,quer_counts,N_sols,numbering):\n",
    "    quers = []\n",
    "    for num in sol:\n",
    "        quers.append(numbering.inverse[num])\n",
    "    return get_n_sols(n,m,c,quers,quer_counts,N_sols)\n",
    "\n",
    "def choose_parents(fit_dict,n_parents):\n",
    "    choices = tuple(fit_dict.keys())\n",
    "    n_sol_lst = []\n",
    "    tot = sum(fit_dict.values())\n",
    "    for choice in choices:\n",
    "        n_sol_lst.append((fit_dict[choice]))\n",
    "    if len(set(n_sol_lst)) == 1:\n",
    "        probs = np.array([1/len(n_sol_lst)]*len(n_sol_lst))\n",
    "    else:\n",
    "        probs = []\n",
    "        max_lst = max(n_sol_lst)\n",
    "        for i in n_sol_lst:\n",
    "            prob = (max_lst - i)/sum([max_lst - x for x in n_sol_lst])\n",
    "            if prob == 0:\n",
    "                prob += 0.001\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "    probs /= probs.sum()\n",
    "    # print(probs)\n",
    "    choices_index = np.random.choice(range(len(choices)), n_parents, replace=False, p=probs)\n",
    "    return [choices[i] for i in choices_index]\n",
    "\n",
    "def breed(parents,n_crossovers,largest_number):\n",
    "    children = []\n",
    "    for i in range(n_crossovers):\n",
    "        choice_index = np.random.choice(range(len(parents)), 2, replace= False)\n",
    "        couple = [parents[i] for i in choice_index]\n",
    "        children += crossover(couple,largest_number)\n",
    "    return children \n",
    "\n",
    "def crossover(couple,largest_number):\n",
    "    parent1 = couple[0]\n",
    "    parent2 = couple[1]\n",
    "    point = np.random.randint(0,len(couple[0])+1)\n",
    "    child1 = parent1[0:point] + parent2[point:]\n",
    "    child2 = parent2[0:point] + parent1[point:]\n",
    "    return [fix_child(child1,largest_number), fix_child(child2,largest_number)]\n",
    "    \n",
    "def fix_child(child,largest_number):\n",
    "    child_set = set(child)\n",
    "    new_choices = np.random.choice(list(set(range(largest_number))-child_set),len(child) - len(child_set), replace= False )\n",
    "    return list(child_set.union(new_choices))\n",
    "\n",
    "def cull(pop,fit_dict,pop_size,n,m,c,quer_counts,N_sols,numbering):\n",
    "    for i_ in pop:\n",
    "        i = tuple(i_)\n",
    "        if i not in fit_dict.keys():\n",
    "            fit_dict[i] = get_fitness(i,n,m,c,quer_counts,N_sols,numbering)\n",
    "    sorted_pop = list(sorted(fit_dict.items(), key=lambda x:x[1]))\n",
    "    culled_pop = [sorted_pop[x][0] for x in range(pop_size)]\n",
    "    new_fit_dict = defaultdict()\n",
    "    for i in culled_pop:\n",
    "        new_fit_dict[i] = fit_dict[i]\n",
    "    return new_fit_dict\n",
    "\n",
    "def mutate(pop,n_mutants,largest_number):\n",
    "    mutant_choice = np.random.choice(range(len(pop)), n_mutants)\n",
    "    mutated = []\n",
    "    for choice in mutant_choice:\n",
    "        ind = pop[choice]\n",
    "        bit_choice = np.random.choice(range(len(ind)))\n",
    "        new_ind = list(ind)\n",
    "        new_quer = int(np.random.choice(list(set(range(largest_number))-set(ind))))\n",
    "        new_ind[bit_choice] = new_quer\n",
    "        mutated.append(new_ind)\n",
    "    return mutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b666cbd-1d9f-4507-b1c9-cfc35a965348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simu_annealing(n_iters,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering):\n",
    "    \n",
    "    largest_number = max_quers(m)\n",
    "    fit_dict = defaultdict()\n",
    "    current_sol = tuple(np.random.choice(list(range(largest_number)), k, replace = False))\n",
    "    fit_dict['current_sol'] = get_fitness(current_sol,n,m,c,quer_counts,N_sols,numbering)\n",
    "    temp = t_0\n",
    "    last_ = [i for i in range(repeats)]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        if i >= n_iters*0.5:\n",
    "            last_[i%repeats] = int(fit_dict['current_sol'])\n",
    "            if len(set(last_)) == 1:\n",
    "                # print(i)\n",
    "                break\n",
    "        new_sol = find_neighbour(current_sol,largest_number)\n",
    "        fit_dict['new_sol'] = get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering)\n",
    "        # print(fit_dict)\n",
    "        change = fit_dict['new_sol'] - fit_dict['current_sol']\n",
    "        # print(change)\n",
    "        p = get_prob(temp,change)\n",
    "        # print(p)\n",
    "        if np.random.uniform() < p:\n",
    "            current_sol = tuple(new_sol)\n",
    "            fit_dict['current_sol'] = int(fit_dict['new_sol'])\n",
    "        temp = temp*alpha\n",
    "    # print(i)\n",
    "    return (tuple(sorted([int(x) for x in current_sol])), get_fitness(current_sol,n,m,c,quer_counts,N_sols,numbering))\n",
    "        \n",
    "def get_prob(temp,change):\n",
    "    if change <= 0:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = (math.e)**(-change/temp)\n",
    "    return p\n",
    "            \n",
    "def find_neighbour(sol,largest_number):\n",
    "    bit_choice = np.random.choice(range(len(sol)))\n",
    "    new_ind = list(sol)\n",
    "    new_quer = int(np.random.choice(list(set(range(largest_number))-set(sol))))\n",
    "    new_ind[bit_choice] = new_quer\n",
    "    return tuple(new_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "dff0c6f5-0d3c-49f8-8c85-75241fb10681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 8, 40, 41, 43, 45, 61, 68, 71, 72], 1000)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "m = 4\n",
    "k = 10\n",
    "c = 1\n",
    "N_paths = 800\n",
    "N_sols = 1000\n",
    "dist = 'uniform'\n",
    "# sol_thresh = 20\n",
    "# bb_thresh = 0.7\n",
    "db = gen_bin_data_set(n,m)\n",
    "quer_seed_dict = query_seed_dict(m)\n",
    "# display(db)\n",
    "quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "# display(quer_counts)\n",
    "\n",
    "#create numbering, doing it outside of function if need to translate numbering to network graph\n",
    "numbering = bidict()\n",
    "i = 0 \n",
    "for quer in gen_queries_all(m):\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "    \n",
    "n_iters_SA = 1000\n",
    "alpha = 0.99\n",
    "t_0 = n_iters + 4\n",
    "repeats = 100\n",
    "simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "50a614d7-5cbc-4334-b08b-3f9a5dfaf88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_quers(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2aef9-1401-457f-84bb-e4bfd6cee3b4",
   "metadata": {},
   "source": [
    "## For a specific instance, printing out paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49bf610d-6a74-4b6d-96da-e95fbecf5f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_0</th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   att_0  att_1  att_2\n",
       "0      1      1      1\n",
       "1      1      1      1\n",
       "2      1      1      1\n",
       "3      0      0      1\n",
       "4      0      0      1\n",
       "5      1      0      1\n",
       "6      1      0      0\n",
       "7      1      0      0\n",
       "8      0      0      1\n",
       "9      0      0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA: ([0, 2, 3, 5, 15, 16, 19, 25], 1) red\n",
      "GA: ([0, 3, 5, 8, 13, 15, 17, 19], 1) green\n",
      "Good Paths: ([0, 1, 3, 5, 6, 11, 15, 19], 1) yellow\n",
      "Large Atts: ([1, 2, 5, 6, 9, 11, 15, 19], 33) pink\n",
      "Avg 50 trials: ([], 205)\n",
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "m = 3\n",
    "k = 8\n",
    "c = 1\n",
    "N_paths = 400\n",
    "N_sols = 1000\n",
    "dist = 'uniform'\n",
    "# sol_thresh = 20\n",
    "# bb_thresh = 0.7\n",
    "db = gen_bin_data_set(n,m)\n",
    "quer_seed_dict = query_seed_dict(m)\n",
    "display(db)\n",
    "quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "# display(quer_counts)\n",
    "\n",
    "#create numbering, doing it outside of function if need to translate numbering to network graph\n",
    "numbering = bidict()\n",
    "i = 0 \n",
    "for quer in gen_queries_all(m):\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "    \n",
    "# display(numbering)\n",
    "    \n",
    "n_iters_GA = 100\n",
    "pop_size = 100\n",
    "n_crossovers = 5\n",
    "n_mutants = 5\n",
    "n_parents = 20\n",
    "n_iters_SA = 1000\n",
    "alpha = 0.95\n",
    "t_0 = n_iters_SA + 4\n",
    "repeats = 200\n",
    "rand_trials = 50\n",
    "\n",
    "SA = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "print('SA:', SA, 'red')\n",
    "\n",
    "\n",
    "GA = genetic_algo(n_iters_GA, pop_size,n_parents, n_crossovers, n_mutants, quer_counts,n,m,c,N_sols,k,numbering)\n",
    "print('GA:', GA, 'green')\n",
    "\n",
    "path_dicts = produce_paths(db,quer_seed_dict,n,m,c,dist,N_sols,k,N_paths,numbering)\n",
    "sol_avg_dict = quer_perf_avg(path_dicts,numbering)\n",
    "new_lst = []\n",
    "for i in list(sorted(sol_avg_dict.items(), key=lambda x:x[1])):\n",
    "    a = list(i)\n",
    "    a.append(quer_counts[numbering.inverse[i[0]]])\n",
    "    new_lst.append(a)\n",
    "selection = []\n",
    "for quer in new_lst[0:k]:\n",
    "    selection.append(numbering.inverse[quer[0]])\n",
    "    \n",
    "good_paths = (sorted(tuple([numbering[x] for x in selection])), get_n_sols(n,m,c,selection,quer_counts,N_sols))\n",
    "print('Good Paths:', good_paths, 'yellow')\n",
    "selection_big_counts = gen_queries(m,n,db,c,quer_seed_dict,k,'large_counts_first',dist)\n",
    "\n",
    "large_atts = (sorted(tuple([numbering[x] for x in selection_big_counts])), \\\n",
    "                      get_n_sols(n,m,c,selection_big_counts,quer_counts,N_sols))\n",
    "print('Large Atts:', large_atts, 'pink')\n",
    "print(f'Avg {rand_trials} trials:', ([],(int(np.mean([get_fitness(np.random.choice(range(max_quers(m)),k),n,m,c,quer_counts,N_sols,numbering) \\\n",
    "                                            for i in range(rand_trials)])))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = Network(notebook = True, height='1000px', width='90%', directed=True,\n",
    "              # layout = True\n",
    "             )\n",
    "# net.add_nodes(list(numbering.values()), label = [str(x) for x in numbering.keys()], level = [0 for x in numbering.keys()])\n",
    "for query in quer_counts.keys():\n",
    "    net.add_node(numbering[query], str(query)+': '+str(quer_counts[query]), \n",
    "                 # level = len(query[0])\n",
    "                )\n",
    "    \n",
    "# edges = get_edges(numbering,m)\n",
    "# net.add_edges(edges)\n",
    "    \n",
    "def my_add_edges(path,colour,network):\n",
    "    for i in range(len(path) - 1):\n",
    "        network.add_edge(int(path[i]),int(path[i+1]), color = colour, value =3)\n",
    "    return None\n",
    "\n",
    "my_add_edges(SA[0],'red',net)\n",
    "my_add_edges(GA[0],'green',net)\n",
    "my_add_edges(good_paths[0],'yellow',net)\n",
    "my_add_edges(large_atts[0],'pink',net)\n",
    "net.set_edge_smooth('dynamic')\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# display(HTML('nodes.html'))\n",
    "# net.show('alg_comp_paths.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ef676-8b4c-437d-a531-3dbebe8694f4",
   "metadata": {},
   "source": [
    "## going to get some average results using this.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff83d36-9707-4cca-b336-1a83ea1f80e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA: ([0, 6, 27, 32, 38, 40, 41, 45, 62, 92, 101, 113, 141, 150, 155, 157, 159, 178, 186, 209], 1) red\n",
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "m = 5\n",
    "k = 20\n",
    "c = 1\n",
    "N_paths = 400\n",
    "N_sols = 1000\n",
    "dist = 'uniform'\n",
    "# sol_thresh = 20\n",
    "# bb_thresh = 0.7\n",
    "db = gen_bin_data_set(n,m)\n",
    "quer_seed_dict = query_seed_dict(m)\n",
    "# display(db)\n",
    "quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "# display(quer_counts)\n",
    "\n",
    "#create numbering, doing it outside of function if need to translate numbering to network graph\n",
    "numbering = bidict()\n",
    "i = 0 \n",
    "for quer in gen_queries_all(m):\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "    \n",
    "n_iters_GA = 100\n",
    "pop_size = 100\n",
    "n_crossovers = 5\n",
    "n_mutants = 5\n",
    "n_parents = 20\n",
    "n_iters_SA = 1000\n",
    "alpha = 0.95\n",
    "t_0 = n_iters_SA + 4\n",
    "repeats = 2\n",
    "rand_trials = 50\n",
    "\n",
    "SA = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "\n",
    "\n",
    "def gen_paths_SA(numbering,N_trials,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k):\n",
    "    \n",
    "    results = defaultdict()\n",
    "    for i in range(N_trials):\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "        \n",
    "        print(quer_counts)\n",
    "    return None\n",
    "\n",
    "\n",
    "# net = Network(notebook = True, height='1000px', width='90%', directed=True,\n",
    "#               # layout = True\n",
    "#              )\n",
    "# # net.add_nodes(list(numbering.values()), label = [str(x) for x in numbering.keys()], level = [0 for x in numbering.keys()])\n",
    "# for query in quer_counts.keys():\n",
    "#     net.add_node(numbering[query], str(query)+': '+str(quer_counts[query]), \n",
    "#                  # level = len(query[0])\n",
    "#                 )\n",
    "    \n",
    "# # edges = get_edges(numbering,m)\n",
    "# # net.add_edges(edges)\n",
    "    \n",
    "# def my_add_edges(path,colour,network):\n",
    "#     for i in range(len(path) - 1):\n",
    "#         network.add_edge(int(path[i]),int(path[i+1]), color = colour, value =3)\n",
    "#     return None\n",
    "\n",
    "# my_add_edges(SA[0],'red',net)\n",
    "# # # my_add_edges(GA[0],'green',net)\n",
    "# # # my_add_edges(good_paths[0],'yellow',net)\n",
    "# # # my_add_edges(large_atts[0],'pink',net)\n",
    "# net.set_edge_smooth('dynamic')\n",
    "# net.show_buttons(filter_=['physics'])\n",
    "# display(HTML('nodes.html'))\n",
    "# net.show('alg_comp_paths_test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "43076c49-3f85-40a6-b980-bedea390d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_paths_SA_additive_k(numbering,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k_low,k_high):\n",
    "    \n",
    "    results = defaultdict()\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    quer_seed_dict = query_seed_dict(m)\n",
    "    quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "    for k in range(k_low,k_high+1):\n",
    "        if k%5 ==1:\n",
    "            print(k)\n",
    "        results[k] = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9ca359de-e425-4fe9-8639-2b658d756f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "61\n",
      "71\n",
      "81\n",
      "91\n",
      "101\n",
      "111\n",
      "121\n",
      "131\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "def gen_paths_SA(numbering,N_trials,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k):\n",
    "    \n",
    "    results = defaultdict()\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    for i in range(N_trials):\n",
    "        if i%10 ==1:\n",
    "            print(i)\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "        results[i] = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "#create numbering, doing it outside of function if need to translate numbering to network graph\n",
    "m = 5\n",
    "numbering = bidict()\n",
    "i = 0 \n",
    "for quer in gen_queries_all(m):\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "n_iters_GA = 100\n",
    "pop_size = 100\n",
    "n_crossovers = 5\n",
    "n_mutants = 5\n",
    "n_parents = 20\n",
    "n_iters_SA = 1000\n",
    "alpha = 0.95\n",
    "t_0 = n_iters_SA + 4\n",
    "repeats = 2\n",
    "rand_trials = 50\n",
    "n = 5\n",
    "k = 20\n",
    "c = 1\n",
    "N_paths = 400\n",
    "N_sols = 1000\n",
    "dist = 'uniform'\n",
    "N_trials = 150\n",
    "k_low = 5\n",
    "k_high = 15\n",
    "data = gen_paths_SA(numbering,N_trials,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k)\n",
    "# data = gen_paths_SA_additive_k(numbering,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k_low,k_high)\n",
    "\n",
    "# print(os.getcwd())\n",
    "filename = f'SA_paths_k_additive/n_{n}_m_{m}_k_l_{k_low}_k_h_{k_high}_N_t_{N_trials}_same_db.json'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f: \n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "81694b32-1718-45e4-952c-21bd0480b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_add_nodes(paths_used,network,res_dict,numbering,node_scaling):\n",
    "    # net.add_node('a', hidden = True, value = 0)\n",
    "    path_nodes = Counter()\n",
    "    # for i in range(min(len(res_dict.values()),paths_used)):\n",
    "    for i in res_dict.keys():\n",
    "        path_nodes.update(res_dict[str(i)][0])\n",
    "        # path_nodes.update([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "    # print(path_nodes)\n",
    "    not_visited = list(set(numbering.values()) - set(path_nodes.keys()))\n",
    "    for query in path_nodes.keys():\n",
    "        net.add_node(query, str(query),\n",
    "                     size=node_scaling*path_nodes[query], \n",
    "                     # value=50, \n",
    "                     color = 'blue', \n",
    "                     title= str(numbering.inverse[query])+ \"\\n\"+ \"visits: \" + str(path_nodes[query])\n",
    "                 # level = len(query[0])\n",
    "                )\n",
    "    for query in not_visited:\n",
    "        net.add_node(query, size=10, color = 'green', title = str(numbering.inverse[query])\n",
    "                 # level = len(query[0])\n",
    "                )\n",
    "    return None\n",
    "\n",
    "def my_add_edges(paths_used,res_dict,colour,network):\n",
    "    # network.add_edge(1,1,value = 0, hidden = True)\n",
    "    # for j in range(min(len(res_dict.values()),paths_used)):\n",
    "    for j in res_dict.keys():\n",
    "        path = res_dict[str(j)][0]\n",
    "        temp_col = \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "        # print(temp_col)\n",
    "        for i in range(len(path) - 1):\n",
    "            # np.random.seed(np.random.choice(range(100000)))\n",
    "            network.add_edge(int(path[i]),int(path[i+1]), color = temp_col, size =0.1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "757c587f-0065-4e9a-86ef-43a0de355785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2filename(path):\n",
    "    return path.lstrip(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3be32c31-418e-400b-9e03-150d587ae2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5': [[5, 6, 13, 15, 25], 1],\n",
       " '6': [[5, 6, 9, 13, 15, 24], 1],\n",
       " '7': [[0, 3, 5, 6, 12, 13, 25], 1],\n",
       " '8': [[5, 6, 11, 13, 15, 20, 22, 23], 1],\n",
       " '9': [[1, 3, 4, 5, 6, 10, 13, 17, 20], 1],\n",
       " '10': [[3, 5, 6, 13, 14, 15, 16, 20, 24, 25], 1],\n",
       " '11': [[2, 6, 12, 13, 14, 15, 17, 21, 22, 23, 24], 1],\n",
       " '12': [[3, 5, 6, 8, 9, 11, 12, 13, 15, 16, 21, 25], 1],\n",
       " '13': [[0, 2, 3, 6, 8, 9, 13, 14, 15, 17, 19, 20, 25], 1],\n",
       " '14': [[3, 4, 6, 7, 8, 9, 10, 13, 15, 17, 18, 20, 21, 25], 1],\n",
       " '15': [[0, 3, 4, 5, 6, 7, 9, 11, 13, 15, 17, 21, 22, 24, 25], 1]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"90%\"\n",
       "            height=\"600px\"\n",
       "            src=\"network_html/small_m_additive_k.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x155d31c40>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'differential_privacy/toy_model_ILP/SA_paths_k_additive/n_5_m_3_k_l_5_k_h_15_N_t_50_same_db.json'\n",
    "f = open(path2filename(path))\n",
    "res_dict = json.load(f)\n",
    "display(res_dict)\n",
    "paths_used = 50\n",
    "\n",
    "net = Network(notebook = True, height='1000px', width='90%', \n",
    "              directed=True,\n",
    "              # filter_menu=True,\n",
    "              # select_menu=True,\n",
    "              # layout = True\n",
    "             )\n",
    "# net.add_nodes(list(numbering.values()), label = [str(x) for x in numbering.keys()], level = [0 for x in numbering.keys()])\n",
    "# for query in quer_counts.keys():\n",
    "#     net.add_node(numbering[query], str(query)+': '+str(quer_counts[query]), \n",
    "#                  # level = len(query[0])\n",
    "#                 )\n",
    "\n",
    "my_add_nodes(paths_used,net,res_dict,numbering)\n",
    "    \n",
    "# edges = get_edges(numbering,m)\n",
    "# net.add_edges(edges)\n",
    "    \n",
    "my_add_edges(paths_used,res_dict,'red',net)\n",
    "\n",
    "# my_add_edges(SA[0],'red',net)\n",
    "# # # my_add_edges(GA[0],'green',net)\n",
    "# # # my_add_edges(good_paths[0],'yellow',net)\n",
    "# # # my_add_edges(large_atts[0],'pink',net)\n",
    "net.set_edge_smooth('dynamic')\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# display(HTML('nodes.html'))\n",
    "net.show('network_html/small_m_additive_k.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0038ef07-829c-4fd6-b972-9e079013eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SA_paths/m_5_k_20_first_try.json'\n",
    "f = open(filename)\n",
    "res_dict = json.load(f)\n",
    "# display(res_dict)\n",
    "paths_used = 50\n",
    "\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "# net = Network(notebook = True, height='1000px', width='90%', \n",
    "#               # directed=True,\n",
    "#               # layout = True\n",
    "#              )\n",
    "# # net.add_nodes(list(numbering.values()), label = [str(x) for x in numbering.keys()], level = [0 for x in numbering.keys()])\n",
    "# # for query in quer_counts.keys():\n",
    "# #     net.add_node(numbering[query], str(query)+': '+str(quer_counts[query]), \n",
    "# #                  # level = len(query[0])\n",
    "# #                 )\n",
    "\n",
    "# my_add_nodes(paths_used,net,res_dict,numbering)\n",
    "    \n",
    "# # edges = get_edges(numbering,m)\n",
    "# # net.add_edges(edges)\n",
    "    \n",
    "# my_add_edges(paths_used,res_dict,'red',net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c2b42723-67da-49ef-bd1c-95e72c9e6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(*(1,3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b65ae6d7-d94b-44c2-9502-b6b6b14f065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "SA_paths/n_5_m_5_k_20_N_t_50_same_db.json\n"
     ]
    }
   ],
   "source": [
    "s = 'differential_privacy/toy_model_ILP/SA_paths/n_5_m_5_k_20_N_t_50_same_db.json'\n",
    "print(len(s))\n",
    "print(s.lstrip(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "65fc8e44-4f72-4969-aebb-310bee2ac205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    }
   ],
   "source": [
    "net = Network(notebook = True, height='1000px', width='90%', \n",
    "              directed=True,\n",
    "              # filter_menu=True,\n",
    "              # select_menu=True,\n",
    "              # layout = True\n",
    "             )\n",
    "# net.add_nodes(list(numbering.values()), label = [str(x) for x in numbering.keys()], level = [0 for x in numbering.keys()])\n",
    "# for query in quer_counts.keys():\n",
    "#     net.add_node(numbering[query], str(query)+': '+str(quer_counts[query]), \n",
    "#                  # level = len(query[0])\n",
    "#                 )\n",
    "\n",
    "my_add_nodes(paths_used,net,res_dicty,numbering)\n",
    "    \n",
    "# edges = get_edges(numbering,m)\n",
    "# net.add_edges(edges)\n",
    "    \n",
    "my_add_edges(paths_used,res_dicty,'red',net)\n",
    "\n",
    "# my_add_edges(SA[0],'red',net)\n",
    "# # # my_add_edges(GA[0],'green',net)\n",
    "# # # my_add_edges(good_paths[0],'yellow',net)\n",
    "# # # my_add_edges(large_atts[0],'pink',net)\n",
    "net.set_edge_smooth('dynamic')\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# display(HTML('nodes.html'))\n",
    "# net.show('network_html/tststststtstestest.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "5eb9c869-d835-48d8-a2bf-f8dd08fe68f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0': ((12, 19, 38, 84, 87, 101, 182, 230), 4), '1': ((30, 40, 41, 54, 66, 87, 95, 230), 1), '2': ((12, 18, 24, 40, 62, 66, 87, 95), 2), '3': ((21, 24, 40, 84, 122, 155, 164, 198), 6), '4': ((1, 8, 84, 120, 138, 156, 164, 230), 82), '5': ((24, 46, 84, 95, 99, 101, 146, 155), 11), '6': ((39, 57, 70, 155, 164, 182, 187, 192), 6), '7': ((12, 30, 38, 39, 66, 70, 87, 90), 3), '8': ((24, 40, 49, 84, 92, 101, 117, 155), 4), '9': ((12, 30, 40, 41, 66, 90, 117, 155), 3), '10': ((7, 12, 40, 53, 84, 87, 123, 230), 3), '11': ((35, 40, 70, 79, 84, 87, 116, 189), 5), '12': ((37, 48, 185, 187, 197, 198, 200, 236), 1000), '13': ((16, 46, 62, 72, 95, 100, 142, 217), 1000), '14': ((46, 66, 77, 87, 115, 158, 197, 230), 2), '15': ((7, 39, 40, 66, 138, 155, 194, 230), 1), '16': ((12, 24, 49, 59, 84, 87, 114, 164), 24), '17': ((38, 39, 44, 59, 66, 116, 155, 230), 1), '18': ((12, 38, 40, 58, 64, 66, 87, 187), 8), '19': ((24, 66, 79, 123, 138, 158, 229, 241), 90), '20': ((40, 67, 84, 99, 155, 164, 224, 230), 4), '21': ((5, 12, 18, 38, 46, 66, 79, 155), 1), '22': ((5, 35, 59, 66, 87, 123, 227, 230), 2), '23': ((7, 13, 22, 46, 78, 102, 118, 216), 1000), '24': ((33, 91, 106, 110, 112, 141, 147, 179), 1000), '25': ((19, 71, 84, 133, 153, 164, 189, 230), 12), '26': ((5, 21, 24, 46, 58, 66, 115, 155), 2), '27': ((12, 38, 39, 62, 84, 87, 116, 230), 4), '28': ((5, 12, 38, 46, 58, 66, 87, 97), 1), '29': ((6, 66, 138, 155, 175, 182, 199, 230), 2)})\n"
     ]
    }
   ],
   "source": [
    "# def noise_SA(numbering,N_trials,dist,n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k):\n",
    "    \n",
    "#     results = defaultdict()\n",
    "#     db = gen_bin_data_set(n,m)\n",
    "#     for i in range(N_trials):\n",
    "#         if i%10 ==1:\n",
    "#             print(i)\n",
    "#         quer_seed_dict = query_seed_dict(m)\n",
    "#         quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "#         results[i] = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "        \n",
    "#     return results\n",
    "\n",
    "\n",
    "#create numbering, doing it outside of function if need to translate numbering to network graph\n",
    "# m = 5\n",
    "# numbering = bidict()\n",
    "# i = 0 \n",
    "# for quer in gen_queries_all(m):\n",
    "#     numbering[quer] = i\n",
    "#     i+=1\n",
    "    \n",
    "# n_iters_GA = 100\n",
    "# pop_size = 100\n",
    "# n_crossovers = 5\n",
    "# n_mutants = 5\n",
    "# n_parents = 20\n",
    "# n_iters_SA = 1000\n",
    "# alpha = 0.95\n",
    "# t_0 = n_iters_SA + 4\n",
    "# repeats = 2\n",
    "# rand_trials = 50\n",
    "# n = 5\n",
    "# k = 8\n",
    "# c = 1\n",
    "# N_paths = 400\n",
    "# N_sols = 1000\n",
    "# dist = 'uniform'\n",
    "# N_trials = 150\n",
    "# k_low = 5\n",
    "# k_high = 15\n",
    "\n",
    "# db = gen_bin_data_set(n,m)\n",
    "# quer_seed_dict = query_seed_dict(m)\n",
    "# quer_counts = get_quer_counts(db,m,c,quer_seed_dict,dist)\n",
    "\n",
    "# res_dicty = defaultdict()\n",
    "for b in range(10):\n",
    "    temp = simu_annealing(n_iters_SA,repeats,alpha,t_0,quer_counts,n,m,c,N_sols,k,numbering)\n",
    "    res_dicty[str(b+20)] = temp\n",
    "print(res_dicty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5d831c2e-2470-41e3-b6ba-93b7047b73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_0</th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   att_0  att_1  att_2  att_3  att_4\n",
       "0      1      0      1      0      0\n",
       "1      0      0      1      1      0\n",
       "2      1      0      0      1      1\n",
       "3      0      0      1      0      0\n",
       "4      1      0      0      0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: ((3,), (1,)): 3\n",
      "18: ((0, 3), (0, 0)): 0\n",
      "39: ((2, 3), (0, 1)): 0\n",
      "40: ((2, 3), (1, 0)): 3\n",
      "66: ((0, 1, 4), (0, 0, 0)): 3\n",
      "70: ((0, 1, 4), (1, 0, 0)): 2\n",
      "155: ((0, 1, 2, 4), (1, 0, 0, 1)): 3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"network_html/large_m_same_db_SA_few_times.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1563cc4f0>"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def paths2pandas(res_dict):\n",
    "    main_lst = []\n",
    "    edge_count = defaultdict(lambda:0)\n",
    "    for path in res_dict.values():\n",
    "        for i in range(len(path[0])-1):\n",
    "            edge = (path[0][i], path[0][i+1])\n",
    "            edge_count[edge] += 1\n",
    "    out_lst = []\n",
    "    for i in edge_count.items():\n",
    "        temp = []\n",
    "        temp.append(i[0][0])\n",
    "        temp.append(i[0][1])\n",
    "        temp.append(i[1])\n",
    "        out_lst.append(temp)\n",
    "\n",
    "    return pd.DataFrame(out_lst, columns=['from', 'to', 'weight']), edge_count\n",
    "\n",
    "# path = 'differential_privacy/toy_model_ILP/SA_paths/n_5_m_5_k_l_5_k_h_15_N_t_150_same_db.json'\n",
    "# f = open(path2filename(path))\n",
    "# res_dict = json.load(f)\n",
    "# display(res_dict)\n",
    "paths_used = 5\n",
    "\n",
    "path_nodes = Counter()\n",
    "# for i in range(min(len(res_dict.values()),paths_used)):\n",
    "for i in res_dicty.keys():\n",
    "    path_nodes.update(res_dicty[str(i)][0])\n",
    "\n",
    "temp_data = paths2pandas(res_dicty)\n",
    "\n",
    "\n",
    "g = nx.Graph()\n",
    "for i, (source, target, weight) in temp_data[0].iterrows():\n",
    "    width = weight\n",
    "    g.add_edge(source, target, size=width, color='blue' if width > 3 else 'black')\n",
    "    \n",
    "# gv.d3(g)\n",
    "nt = Network('500px', '100%',notebook = True)\n",
    "\n",
    "sources = temp_data[0]['from']\n",
    "targets = temp_data[0]['to']\n",
    "weights = temp_data[0]['weight']\n",
    "\n",
    "edge_data = zip(sources, targets, weights)\n",
    "\n",
    "m=5\n",
    "numbering = bidict()\n",
    "i = 0 \n",
    "for quer in gen_queries_all(m):\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "# print(numbering.inverse)\n",
    "\n",
    "for e in edge_data:\n",
    "    src = e[0]\n",
    "    dst = e[1]\n",
    "    w = str(e[2])\n",
    "\n",
    "    nt.add_node(src, str(src), title=str(numbering.inverse[src])+ ': '+ str(quer_counts[numbering.inverse[src]]) + '\\n' + 'visits: ' + str(path_nodes[src]) + '\\n')\n",
    "    nt.add_node(dst, str(dst), title=str(numbering.inverse[dst])+ ': '+ str(quer_counts[numbering.inverse[dst]]) + '\\n' + 'visits: ' + str(path_nodes[dst]) + '\\n')\n",
    "    nt.add_edge(src, dst, value=w, title = temp_data[1][(src,dst)])\n",
    "\n",
    "neighbor_map = nt.get_adj_list()\n",
    "# print(neighbor_map)\n",
    "\n",
    "# add neighbor data to node hover data\n",
    "for node in nt.nodes:\n",
    "    node[\"title\"] += \" Neighbors: \\n\" + \"\\n\".join([str(x) for x in neighbor_map[node[\"id\"]]])\n",
    "    # node[\"title\"] += \" Neighbors: \\n\" + \"\\n\".join(str(neighbor_map[node[\"id\"]]))\n",
    "    # node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
    "    node[\"value\"] = path_nodes[node[\"id\"]]\n",
    "\n",
    "display(db)\n",
    "# print(quer_counts[(2,4),()])\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "new_sol = [7, 18, 39, 40, 66, 70, 155]\n",
    "for i in new_sol: print(str(i) +': '+ str(numbering.inverse[i])+ ': '+ str(quer_counts[numbering.inverse[i]]))\n",
    "print(get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering))\n",
    "# new_sol = [7, 18, 39, 40, 66, 70, 148,155]\n",
    "# print(get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering))\n",
    "nt.show(\"network_html/large_m_same_db_SA_few_times.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "86c390c3-38bf-4e16-89b3-4a8d00222b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = f'small_example_data/n_{n}_m_{m}_example_1_quer_counts.json'\n",
    "# os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "# with open(filename, 'w') as f: \n",
    "#     json.dump(dict((str(x),quer_counts[x]) for x in quer_counts.keys()), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "40875bc1-f4a7-489c-bc6d-78c4786d0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_alt_paths(sol,T_max,n,m,c,quer_counts,N_sols,numbering):\n",
    "    largest_number = max_quers(m)\n",
    "    current_n_sols = get_fitness(sol,n,m,c,quer_counts,N_sols,numbering)\n",
    "    alt_sols = []\n",
    "    pop = [list(sol)]\n",
    "    curr_time = t.time()\n",
    "    while t.time() <= curr_time + T_max:\n",
    "        new_sol = mutate(pop,1,largest_number)[0]\n",
    "        if get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering) <= current_n_sols:\n",
    "            alt_sols.append(tuple(new_sol))\n",
    "    return set(alt_sols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "f37ca7c9-b5b8-419c-b6d7-134afbe32e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666322350.975565\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "print(t.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "ad17b21d-101e-4c15-ae68-6e608c7b111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt_sols = produce_alt_paths([7, 18, 39, 40, 66, 70, 155],60,n,m,c,quer_counts,N_sols,numbering)\n",
    "# display(alt_sols)\n",
    "temp_dicty = defaultdict()\n",
    "mm = 0\n",
    "for sol in alt_sols:\n",
    "    temp_dicty[str(mm)] = (tuple(sorted(sol)), 1)\n",
    "    mm+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "74982a77-e20e-4a02-80b1-cd37554a49cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92: ((0, 3, 4), (0, 1, 0)): 2\n",
      "38: ((2, 3), (0, 0)): 0\n",
      "189: ((0, 2, 3, 4), (1, 0, 1, 1)): 2\n",
      "62: ((0, 1, 3), (1, 0, 0)): 1\n",
      "229: ((0, 1, 2, 3, 4), (1, 0, 0, 1, 1)): 2\n",
      "164: ((0, 1, 3, 4), (0, 0, 1, 0)): 2\n",
      "6: ((3,), (0,)): 2\n",
      "63: ((0, 1, 3), (1, 0, 1)): 2\n",
      "58: ((0, 1, 3), (0, 0, 0)): 0\n",
      "77: ((0, 2, 3), (0, 1, 1)): 2\n",
      "230: ((0, 1, 2, 3, 4), (1, 0, 1, 0, 0)): 2\n",
      "187: ((0, 2, 3, 4), (1, 0, 0, 1)): 0\n",
      "227: ((0, 1, 2, 3, 4), (1, 0, 0, 0, 1)): 0\n",
      "230: ((0, 1, 2, 3, 4), (1, 0, 1, 0, 0)): 2\n",
      "49: ((3, 4), (1, 1)): 2\n",
      "21: ((0, 3), (1, 1)): 2\n",
      "30: ((1, 3), (0, 0)): 2\n",
      "197: ((1, 2, 3, 4), (0, 0, 1, 1)): 2\n"
     ]
    }
   ],
   "source": [
    "temp_lst = []\n",
    "for i in list(alt_sols):\n",
    "    for j in list(set(i) - set([7, 18, 39, 40, 66, 70, 155])):\n",
    "        print(str(j) +': '+ str(numbering.inverse[j])+ ': '+ str(quer_counts[numbering.inverse[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "1f2e7618-805e-458f-83dd-4c1ae74a4b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'0': ((7, 39, 40, 66, 70, 92, 155), 1),\n",
       "             '1': ((18, 38, 39, 40, 66, 70, 155), 1),\n",
       "             '2': ((18, 39, 40, 66, 70, 155, 189), 1),\n",
       "             '3': ((18, 39, 40, 62, 66, 70, 155), 1),\n",
       "             '4': ((18, 39, 40, 66, 70, 155, 229), 1),\n",
       "             '5': ((7, 39, 40, 66, 70, 155, 164), 1),\n",
       "             '6': ((6, 18, 39, 40, 66, 70, 155), 1),\n",
       "             '7': ((18, 39, 40, 63, 66, 70, 155), 1),\n",
       "             '8': ((7, 39, 40, 58, 66, 70, 155), 1),\n",
       "             '9': ((7, 39, 40, 66, 70, 77, 155), 1),\n",
       "             '10': ((7, 18, 39, 40, 66, 155, 230), 1),\n",
       "             '11': ((18, 39, 40, 66, 70, 155, 187), 1),\n",
       "             '12': ((18, 39, 40, 66, 70, 155, 227), 1),\n",
       "             '13': ((7, 39, 40, 66, 70, 155, 230), 1),\n",
       "             '14': ((18, 39, 40, 49, 66, 70, 155), 1),\n",
       "             '15': ((18, 21, 39, 40, 66, 70, 155), 1),\n",
       "             '16': ((18, 30, 39, 40, 66, 70, 155), 1),\n",
       "             '17': ((18, 39, 40, 66, 70, 155, 197), 1)})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(temp_dicty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "726110d7-bbf0-46f5-9436-6e8a4630a3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_0</th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   att_0  att_1  att_2  att_3  att_4\n",
       "0      1      0      1      0      0\n",
       "1      0      0      1      1      0\n",
       "2      1      0      0      1      1\n",
       "3      0      0      1      0      0\n",
       "4      1      0      0      0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: ((3,), (1,)): 3\n",
      "18: ((0, 3), (0, 0)): 0\n",
      "39: ((2, 3), (0, 1)): 0\n",
      "40: ((2, 3), (1, 0)): 3\n",
      "66: ((0, 1, 4), (0, 0, 0)): 3\n",
      "70: ((0, 1, 4), (1, 0, 0)): 2\n",
      "155: ((0, 1, 2, 4), (1, 0, 0, 1)): 3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"small_example_data/minimal_paths_test.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x161598bb0>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def paths2pandas(res_dict):\n",
    "    main_lst = []\n",
    "    edge_count = defaultdict(lambda:0)\n",
    "    for path in res_dict.values():\n",
    "        for i in range(len(path[0])-1):\n",
    "            edge = (path[0][i], path[0][i+1])\n",
    "            edge_count[edge] += 1\n",
    "    out_lst = []\n",
    "    for i in edge_count.items():\n",
    "        temp = []\n",
    "        temp.append(i[0][0])\n",
    "        temp.append(i[0][1])\n",
    "        temp.append(i[1])\n",
    "        out_lst.append(temp)\n",
    "\n",
    "    return pd.DataFrame(out_lst, columns=['from', 'to', 'weight']), edge_count\n",
    "\n",
    "# path = 'differential_privacy/toy_model_ILP/SA_paths/n_5_m_5_k_l_5_k_h_15_N_t_150_same_db.json'\n",
    "# f = open(path2filename(path))\n",
    "# res_dict = json.load(f)\n",
    "# display(res_dict)\n",
    "# paths_used = 5\n",
    "\n",
    "path_nodes = Counter()\n",
    "# for i in range(min(len(res_dict.values()),paths_used)):\n",
    "for i in temp_dicty.keys():\n",
    "    path_nodes.update(temp_dicty[str(i)][0])\n",
    "\n",
    "temp_data = paths2pandas(temp_dicty)\n",
    "\n",
    "\n",
    "g = nx.Graph()\n",
    "for i, (source, target, weight) in temp_data[0].iterrows():\n",
    "    width = weight\n",
    "    g.add_edge(source, target, size=width, color='blue' if width > 3 else 'black')\n",
    "    \n",
    "# gv.d3(g)\n",
    "nt = Network('500px', '100%',notebook = True)\n",
    "\n",
    "sources = temp_data[0]['from']\n",
    "targets = temp_data[0]['to']\n",
    "weights = temp_data[0]['weight']\n",
    "\n",
    "edge_data = zip(sources, targets, weights)\n",
    "\n",
    "m=5\n",
    "numbering = bidict()\n",
    "i = 0 \n",
    "for quer in gen_queries_all(m):\n",
    "    numbering[quer] = i\n",
    "    i+=1\n",
    "# print(numbering.inverse)\n",
    "\n",
    "for e in edge_data:\n",
    "    src = e[0]\n",
    "    dst = e[1]\n",
    "    w = str(e[2])\n",
    "\n",
    "    nt.add_node(src, str(src), title=str(numbering.inverse[src])+ ': '+ str(quer_counts[numbering.inverse[src]]) + '\\n' + 'visits: ' + str(path_nodes[src]) + '\\n')\n",
    "    nt.add_node(dst, str(dst), title=str(numbering.inverse[dst])+ ': '+ str(quer_counts[numbering.inverse[dst]]) + '\\n' + 'visits: ' + str(path_nodes[dst]) + '\\n')\n",
    "    nt.add_edge(src, dst, value=w, title = temp_data[1][(src,dst)])\n",
    "\n",
    "neighbor_map = nt.get_adj_list()\n",
    "# print(neighbor_map)\n",
    "\n",
    "# add neighbor data to node hover data\n",
    "for node in nt.nodes:\n",
    "    node[\"title\"] += \" Neighbors: \\n\" + \"\\n\".join([str(x) for x in neighbor_map[node[\"id\"]]])\n",
    "    # node[\"title\"] += \" Neighbors: \\n\" + \"\\n\".join(str(neighbor_map[node[\"id\"]]))\n",
    "    # node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
    "    node[\"value\"] = path_nodes[node[\"id\"]]\n",
    "\n",
    "display(db)\n",
    "# print(quer_counts[(2,4),()])\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "new_sol = [7, 18, 39, 40, 66, 70, 155]\n",
    "for i in new_sol: print(str(i) +': '+ str(numbering.inverse[i])+ ': '+ str(quer_counts[numbering.inverse[i]]))\n",
    "print(get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering))\n",
    "# new_sol = [7, 18, 39, 40, 66, 70, 148,155]\n",
    "# print(get_fitness(new_sol,n,m,c,quer_counts,N_sols,numbering))\n",
    "nt.show(\"small_example_data/minimal_paths_test.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "52a01a7b-fa36-47d2-a7d2-f823110dd33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"90%\"\n",
       "            height=\"600px\"\n",
       "            src=\"small_example_data/minimal_paths_test_2.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x16015d310>"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_used = 50\n",
    "net = Network(notebook = True, height='1000px', width='90%', \n",
    "              directed=True,\n",
    "              # filter_menu=True,\n",
    "              # select_menu=True,\n",
    "              # layout = True\n",
    "             )\n",
    "# net.add_nodes(list(numbering.values()), label = [str(x) for x in numbering.keys()], level = [0 for x in numbering.keys()])\n",
    "# for query in quer_counts.keys():\n",
    "#     net.add_node(numbering[query], str(query)+': '+str(quer_counts[query]), \n",
    "#                  # level = len(query[0])\n",
    "#                 )\n",
    "\n",
    "my_add_nodes(paths_used,net,temp_dicty,numbering,2)\n",
    "    \n",
    "# edges = get_edges(numbering,m)\n",
    "# net.add_edges(edges)\n",
    "    \n",
    "my_add_edges(paths_used,temp_dicty,'red',net)\n",
    "\n",
    "# my_add_edges(SA[0],'red',net)\n",
    "# # # my_add_edges(GA[0],'green',net)\n",
    "# # # my_add_edges(good_paths[0],'yellow',net)\n",
    "# # # my_add_edges(large_atts[0],'pink',net)\n",
    "net.set_edge_smooth('dynamic')\n",
    "net.show_buttons(filter_=['physics'])\n",
    "# display(HTML('nodes.html'))\n",
    "net.show(\"small_example_data/minimal_paths_test_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f36e25b1-fc84-4330-8a49-80325839c364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(6, 18, 39, 40, 66, 70, 155),\n",
       " (7, 18, 39, 40, 66, 230, 155),\n",
       " (7, 58, 39, 40, 66, 70, 155),\n",
       " (7, 77, 39, 40, 66, 70, 155),\n",
       " (7, 92, 39, 40, 66, 70, 155),\n",
       " (7, 164, 39, 40, 66, 70, 155),\n",
       " (7, 230, 39, 40, 66, 70, 155),\n",
       " (21, 18, 39, 40, 66, 70, 155),\n",
       " (30, 18, 39, 40, 66, 70, 155),\n",
       " (38, 18, 39, 40, 66, 70, 155),\n",
       " (49, 18, 39, 40, 66, 70, 155),\n",
       " (62, 18, 39, 40, 66, 70, 155),\n",
       " (63, 18, 39, 40, 66, 70, 155),\n",
       " (187, 18, 39, 40, 66, 70, 155),\n",
       " (189, 18, 39, 40, 66, 70, 155),\n",
       " (197, 18, 39, 40, 66, 70, 155),\n",
       " (227, 18, 39, 40, 66, 70, 155),\n",
       " (229, 18, 39, 40, 66, 70, 155)}"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_sols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "b8c3145b-7c49-4521-bdfb-413d25bfed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_counts = get_quer_counts(db,m,0,quer_seed_dict,dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "df397555-f6a7-4bfb-81c5-6955fc1f7379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(<function __main__.<lambda>()>, {-1: 1, 1: 7}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 6, -1: 2}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 5, -1: 3}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 6, -1: 2}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 7, 0: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 8}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 6, -1: 1, 0: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {-1: 4, 1: 4}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 5, -1: 3}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 6, -1: 2}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 7, -1: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 5, -1: 3}),\n",
       " defaultdict(<function __main__.<lambda>()>, {-1: 1, 1: 5, 0: 2})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_lst = []\n",
    "for trial in res_dicty.values():\n",
    "    noise_output_dict = defaultdict(lambda: 0)\n",
    "    if trial[1] <= 3:\n",
    "        for i in trial[0]:\n",
    "            noise_output_dict[quer_counts[numbering.inverse[i]] - tru_counts[numbering.inverse[i]]] += 1\n",
    "        out_lst.append(noise_output_dict)\n",
    "display(out_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "95e0a47f-bbb3-4df8-99d6-773dcb388f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(<function __main__.<lambda>()>, {2: 3, 3: 4, 5: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {2: 4, 3: 4}),\n",
       " defaultdict(<function __main__.<lambda>()>, {2: 4, 3: 4}),\n",
       " defaultdict(<function __main__.<lambda>()>, {2: 4, 3: 3, 4: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 1, 2: 2, 3: 4, 5: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {2: 1, 3: 4, 4: 2, 5: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 1, 2: 2, 3: 1, 4: 3, 5: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {2: 3, 3: 3, 4: 1, 5: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 1, 2: 4, 3: 2, 4: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 1, 2: 1, 3: 4, 5: 2}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 1, 2: 3, 3: 3, 4: 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 1, 2: 3, 3: 4}),\n",
       " defaultdict(<function __main__.<lambda>()>, {1: 1, 3: 1, 4: 5, 5: 1})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_lst = []\n",
    "for trial in res_dicty.values():\n",
    "    noise_output_dict = defaultdict(lambda: 0)\n",
    "    if trial[1] <= 3:\n",
    "        for i in trial[0]:\n",
    "            noise_output_dict[len(numbering.inverse[i][0])] += 1\n",
    "        out_lst.append(noise_output_dict)\n",
    "display(out_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba821e-732b-4263-a36c-cbb5f69925f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lst = []\n",
    "for trial in res_dicty.values():\n",
    "    noise_output_dict = defaultdict(lambda: 0)\n",
    "    if trial[1] <= 3:\n",
    "        for i in trial[0]:\n",
    "            noise_output_dict[len(numbering.inverse[i][0])] += 1\n",
    "        out_lst.append(noise_output_dict)\n",
    "display(out_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
