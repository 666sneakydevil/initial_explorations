{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c29ba360-5460-4173-b3c2-461e46b15002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import io\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from itertools import combinations\n",
    "from random import sample\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": ['Computer Modern Roman'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca3f2d06-15c8-4e00-9dcb-b3aa4f6e3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sols_var_n(n_range,m,quer_dict,partial_dict,noise,n_sols):\n",
    "    \"\"\" builds ILP to construct candidate database, based on answers to queries \n",
    "    Assumes bound on noise, n and m is known. \n",
    "    \n",
    "    havent changed anything to this yet****************\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    # Initialize the decision variables\n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "                   for j in range(m)] for i in range(n)])\n",
    "    # print(x)\n",
    "    \n",
    "    #adding the constraints...\n",
    "    quer_cons(M,x,n,quer_dict,noise)\n",
    "    partial_cons(M,x,n,partial_dict)\n",
    "            \n",
    "    # for i in range(n-1):\n",
    "    #     M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    bin_cons(M,x,n)\n",
    "\n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = n_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    \n",
    "    # print(f\"Took {M.Runtime:.2f} seconds to solve\")\n",
    "    \n",
    "    if M.solCount == 0:\n",
    "        print(\"infeasible\")\n",
    "    \n",
    "    out_lst = []\n",
    "    for k in range(M.SolCount):\n",
    "        M.Params.SolutionNumber = k\n",
    "        out_x = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[0])):\n",
    "                out_x[i][j] = x[i][j].Xn\n",
    "        # print([var.Xn for var in m.getVars()])\n",
    "        out_lst.append(out_x)\n",
    "        \n",
    "    return out_lst, M.Runtime\n",
    "\n",
    "def gen_sols(n,m,quer_dict,partial_dict,noise,n_sols):\n",
    "    \"\"\" builds ILP to construct candidate database, based on answers to queries \n",
    "    Assumes bound on noise, n and m is known. \"\"\"\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    # Initialize the decision variables\n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "                   for j in range(m)] for i in range(n)])\n",
    "    # print(x)\n",
    "    \n",
    "    #adding the constraints...\n",
    "    quer_cons(M,x,n,quer_dict,noise)\n",
    "    partial_cons(M,x,n,partial_dict)\n",
    "            \n",
    "    # for i in range(n-1):\n",
    "    #     M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    bin_cons(M,x,n)\n",
    "\n",
    "    # Parameters\n",
    "    M.Params.PoolSearchMode = 2\n",
    "    M.Params.PoolSolutions = n_sols\n",
    "    # m.Params.PoolSolutions = 2\n",
    "    M.Params.PoolGap = 0.0\n",
    "\n",
    "    # Optimize\n",
    "    M.optimize()\n",
    "    \n",
    "    # print(f\"Took {M.Runtime:.2f} seconds to solve\")\n",
    "    \n",
    "    if M.solCount == 0:\n",
    "        print(\"infeasible\")\n",
    "    \n",
    "    out_lst = []\n",
    "    for k in range(M.SolCount):\n",
    "        M.Params.SolutionNumber = k\n",
    "        out_x = np.zeros_like(x)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[0])):\n",
    "                out_x[i][j] = x[i][j].Xn\n",
    "        # print([var.Xn for var in m.getVars()])\n",
    "        out_lst.append(out_x)\n",
    "        \n",
    "    return out_lst, M.Runtime\n",
    "\n",
    "def quer_cons(M,x,n,quer_dict,noise):\n",
    "    \"\"\" Adds the constraints based on the queries in quer_dict\"\"\"\n",
    "    \n",
    "    y_var_dict = defaultdict()\n",
    "    y_p_var_dict = defaultdict()\n",
    "    y_n_var_dict = defaultdict()\n",
    "    \n",
    "    for quer in quer_dict.keys():\n",
    "        y_var_dict[quer] = []\n",
    "        y_p_var_dict[quer] = []\n",
    "        y_n_var_dict[quer] = []\n",
    "        \n",
    "        positions_0 = []\n",
    "        positions_1 = []\n",
    "        for i, j in zip(quer[0],quer[1]):\n",
    "            if j == 0: \n",
    "                positions_0.append(i)\n",
    "            elif j ==1:\n",
    "                positions_1.append(i)\n",
    "        for i in range(n):\n",
    "            y_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_{quer}_{i}\"))\n",
    "            y_p_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_p_{quer}_{i}\"))\n",
    "            y_n_var_dict[quer].append(M.addVar(vtype='B', name=f\"y_n_{quer}_{i}\"))\n",
    "            \n",
    "            M.addConstr( len(quer[0])*y_var_dict[quer][i] + (len(quer[0])+0.5)*y_p_var_dict[quer][i] <= \n",
    "                        sum([x[i][j] for j in positions_1]) + sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( (len(quer[0])-0.5)*y_n_var_dict[quer][i] + len(quer[0])*y_var_dict[quer][i] + \n",
    "                        m*y_p_var_dict[quer][i] >= \n",
    "                        sum([x[i][j] for j in positions_1])+ sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( y_var_dict[quer][i] + y_p_var_dict[quer][i] + y_n_var_dict[quer][i] == 1)\n",
    "            \n",
    "        if noise == 0:\n",
    "            M.addConstr(sum(y_var_dict[quer]) == quer_dict[quer])\n",
    "        else:\n",
    "            M.addConstr(sum(y_var_dict[quer]) <= quer_dict[quer] + noise)\n",
    "            M.addConstr(sum(y_var_dict[quer]) >= quer_dict[quer] - noise)\n",
    "    return None\n",
    "\n",
    "def partial_cons(M,x,n,partial_dict):\n",
    "    \"\"\" Adds the constraints based on the partial information in partial_dict\"\"\"\n",
    "    \n",
    "    y_var_dict_partial = defaultdict()\n",
    "    y_p_var_dict_partial = defaultdict()\n",
    "    y_n_var_dict_partial = defaultdict()\n",
    "    \n",
    "    for quer in partial_dict.keys():\n",
    "        y_var_dict_partial[quer] = []\n",
    "        y_p_var_dict_partial[quer] = []\n",
    "        y_n_var_dict_partial[quer] = []\n",
    "        \n",
    "        positions_0 = []\n",
    "        positions_1 = []\n",
    "        for i, j in zip(quer[0],quer[1]):\n",
    "            if j == 0: \n",
    "                positions_0.append(i)\n",
    "            elif j ==1:\n",
    "                positions_1.append(i)\n",
    "        for i in range(n):\n",
    "            y_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_partial_{quer}_{i}\"))\n",
    "            y_p_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_p_partial_{quer}_{i}\"))\n",
    "            y_n_var_dict_partial[quer].append(M.addVar(vtype='B', name=f\"y_n_partial_{quer}_{i}\"))\n",
    "            \n",
    "            M.addConstr( len(quer[0])*y_var_dict_partial[quer][i] + (len(quer[0])+0.5)*y_p_var_dict_partial[quer][i] <= \n",
    "                        sum([x[i][j] for j in positions_1]) + sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( (len(quer[0])-0.5)*y_n_var_dict_partial[quer][i] + len(quer[0])*y_var_dict_partial[quer][i] + \n",
    "                        m*y_p_var_dict_partial[quer][i] >= \n",
    "                        sum([x[i][j] for j in positions_1])+ sum([(1-x[i][j]) for j in positions_0]))\n",
    "            M.addConstr( y_var_dict_partial[quer][i] + y_p_var_dict_partial[quer][i] + y_n_var_dict_partial[quer][i] == 1)\n",
    "            \n",
    "        # this line does the work\n",
    "        M.addConstr(sum(y_var_dict_partial[quer]) >= partial_dict[quer])\n",
    "        \n",
    "    return None\n",
    "\n",
    "def bin_cons(M,x,n):\n",
    "    \"\"\" Adds binary symmetry breaking constraint \"\"\"\n",
    "    for i in range(n-1):\n",
    "        M.addConstr(binatodeci(x[i]) <= binatodeci(x[i+1]))\n",
    "    return None\n",
    "\n",
    "def gen_bin_data_set(n,m):\n",
    "    \"\"\"n is number of people, m is number of attributes used, database is uniform random\"\"\"\n",
    "    db = pd.DataFrame(np.random.randint(0,2,size=(n, m)), columns=[f'att_{x}' for x in range(m)])\n",
    "    return db\n",
    "\n",
    "def gen_powerset(k):\n",
    "    \"\"\" generates a powerset of k elements \"\"\"\n",
    "    out = []\n",
    "    for i in itertools.product([0,1],repeat=k):\n",
    "        out.append(i)\n",
    "    return out\n",
    "\n",
    "def gen_queries_uniform_complete(m,n_queries):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return sample(queries,min(n_queries,len(queries)))\n",
    "\n",
    "def gen_queries_comp(m):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            for choice in gen_powerset(i):\n",
    "                quer = []\n",
    "                quer.append(comb)\n",
    "                quer.append(choice)\n",
    "                queries.append(tuple(quer))\n",
    "                \n",
    "    return queries\n",
    "\n",
    "def gen_queries_uniform(m,n_queries):\n",
    "    \"\"\"Generates random queries. Chooses number of attributes uniformly over [1,m],\n",
    "    Would be interested to see how this changes with a skewed distribution, also\n",
    "    would be interesting how it changes if symmetry in queries is forced\"\"\"\n",
    "    \n",
    "    queries = []\n",
    "\n",
    "    for i in range(n_queries):\n",
    "        quer = []\n",
    "        atts = np.random.randint(1,m+1)\n",
    "        quer.append(tuple(sorted(sample(range(m),atts))))\n",
    "        quer.append(tuple(np.random.choice([0,1], size=[atts])))\n",
    "        queries.append(quer)\n",
    "\n",
    "    queries = list(set([tuple(x) for x in queries]))\n",
    "\n",
    "    return queries\n",
    "\n",
    "def gen_partial_info(db,n_part,m_part):\n",
    "    \"\"\" generates a partial info dict based on n_partials rows\n",
    "    and m_partials attributes per individual, if no partial info, we get None type \"\"\"\n",
    "    \n",
    "    n = db.shape[0]\n",
    "    m = db.shape[1]\n",
    "    \n",
    "    partial_dict = defaultdict(lambda:0)\n",
    "    \n",
    "    if n_part == 0 or m_part == 0:\n",
    "        return partial_dict\n",
    "    \n",
    "    row_inds = np.random.choice(n,n_part, replace = False)\n",
    "    for i in row_inds:\n",
    "        att_sample = np.random.choice(m,m_part, replace = False)\n",
    "        lst = []\n",
    "        for j in att_sample:\n",
    "            lst.append(int(db[i:i+1][f\"att_{j}\"]))\n",
    "            \n",
    "        # print(att_sample,lst)\n",
    "        vals = [x for _,x in sorted(zip(att_sample,lst))]\n",
    "        \n",
    "        partial_dict[(tuple(sorted(att_sample)),tuple(vals))] += 1\n",
    "    return partial_dict\n",
    "            \n",
    "def quer2string(query):\n",
    "    \"\"\" converts query to a string to be used by pandas 'query' function\"\"\"\n",
    "    string = \"\"\n",
    "    for pos, att in enumerate(query[0]):\n",
    "        string += f'att_{att} == {query[1][pos]} & '\n",
    "    return string[0:-3]\n",
    "\n",
    "def get_counts_uniform(db,quers,noise):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + np.random.randint(-noise,noise+1),0)\n",
    "    return out_dict\n",
    "\n",
    "def binatodeci(binary):\n",
    "    return sum(val*(2**idx) for idx, val in enumerate(reversed(binary)))\n",
    "\n",
    "def check_fixed_sols(sols):\n",
    "    \"\"\"Takes some candidate databases and returns which\n",
    "    rows are contained in all of them, including duplicates\"\"\"\n",
    "    \n",
    "    if len(sols) == 1:\n",
    "        return [tuple(x) for x in sols[0]]\n",
    "    \n",
    "    out_lst = []\n",
    "    for sol in sols:\n",
    "        temp = []\n",
    "        for row in sol:\n",
    "            temp.append(tuple(row))\n",
    "        out_lst.append(temp)\n",
    "    # print(out_lst)\n",
    "\n",
    "    fixed_sols = []\n",
    "    for row in out_lst[0]:\n",
    "        count = 0\n",
    "        for sol in out_lst[1:]:\n",
    "            if row in sol:\n",
    "                sol.remove(row)\n",
    "                count += 1\n",
    "            # else:\n",
    "            #     break\n",
    "        if count == len(sols) - 1:\n",
    "            fixed_sols.append(row)\n",
    "    return fixed_sols\n",
    "\n",
    "def total_sim(sol,db):\n",
    "    count = 0\n",
    "    for i, val_i in enumerate(db):\n",
    "        for j, val_j in enumerate(val_i):\n",
    "            # print(val_j, sol[i][j])\n",
    "            if val_j != sol[i][j]:\n",
    "                count += 1\n",
    "            \n",
    "    return  1 - count/(len(db[0])*len(db))\n",
    "\n",
    "def gen_cij(db,sol,i,j):\n",
    "    \"\"\" computes cost of assigning row i of solution \n",
    "    to row j of database in terms of L1 norm\"\"\"\n",
    "    \n",
    "    cost = sum([abs(db[j][k]-sol[i][k]) for k in range(len(sol[0]))])\n",
    "    return cost\n",
    "\n",
    "def ass_ILP(db,sol):\n",
    "    \"\"\" Creates an optimal assignment of rows in the solution \n",
    "    to the rows in the true database, the objective function value\n",
    "    is returned and is the smallest number of differences between\n",
    "    solution and true database, based on row swapping\"\"\"\n",
    "    \n",
    "    n = len(sol)\n",
    "    M = gp.Model()\n",
    "    M.Params.OutputFlag = 0\n",
    "    \n",
    "    x = np.array([[M.addVar(vtype='B', name=f\"x_{i}_{j}\") \n",
    "               for j in range(n)] for i in range(n)])\n",
    "    \n",
    "    for i in range(n):\n",
    "        M.addConstr(sum(x[i,:]) == 1)\n",
    "        M.addConstr(sum(x[:,i]) == 1)\n",
    "        \n",
    "    M.setObjective(sum([sum([x[i][j]*gen_cij(db,sol,i,j) for j in range(n)]) for i in range(n)]), GRB.MINIMIZE)\n",
    "    \n",
    "    M.optimize()\n",
    "    \n",
    "    out_x = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            out_x[i][j] = x[i][j].Xn\n",
    "    # print(out_x)\n",
    "    \n",
    "    return 1 - M.ObjVal/(len(db)*len(db[0]))\n",
    "\n",
    "def get_counts_uniform_seed(db,quers,noise,quer_seed_dict):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + quer2noise_uniform(query,quer_seed_dict,noise),0)\n",
    "        # print(np.random.randint(-noise,noise+1))\n",
    "    return out_dict\n",
    "\n",
    "def get_counts_triangle_seed(db,quers,noise,quer_seed_dict):\n",
    "    \"\"\"Returns noisy count of query, noise is symmetric rounded triangle [-noise, +noise]\n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + quer2noise_triangle(query,quer_seed_dict,noise),0)\n",
    "        # print(np.random.randint(-noise,noise+1))\n",
    "    return out_dict\n",
    "\n",
    "def get_counts_triangle(db,quers,noise):\n",
    "    \"\"\"Returns noisy count of query, noise is uniform random integer over -noise to noise (inclusive). \n",
    "    Will perturb negative counts to 0.\"\"\"\n",
    "    out_dict = defaultdict(int)\n",
    "    for query in quers:\n",
    "        if noise == 0:\n",
    "            actual_noise = 0\n",
    "        else:\n",
    "            actual_noise = round(np.random.triangular(-noise, 0, noise))\n",
    "        out_dict[tuple(query)] = max(len(db.query(quer2string(query))) + actual_noise,0)\n",
    "    return out_dict\n",
    "\n",
    "def quer2noise_uniform(query,quer_seed_dict,noise):\n",
    "    \"\"\" generates noise from discrete-RV: U({-noise,noise})\n",
    "    using the seed in the query dict\"\"\"\n",
    "    # print(query)\n",
    "    if quer_seed_dict != None:\n",
    "        np.random.seed(quer_seed_dict[query])\n",
    "    return np.random.randint(-noise,noise+1)\n",
    "\n",
    "def quer2noise_triangle(query,quer_seed_dict,noise):\n",
    "    \"\"\" generates noise from rounded triangle distribution over -noise to +noise, median at 0,\n",
    "    using the seed in the query dict\"\"\"\n",
    "    # print(query)\n",
    "    if quer_seed_dict != None:\n",
    "        np.random.seed(quer_seed_dict[query])\n",
    "    return round(np.random.triangular(-noise, 0, noise))\n",
    "\n",
    "def gen_powerset_test(k):\n",
    "    \"\"\" generates a powerset of k elements \"\"\"\n",
    "    out = []\n",
    "    for i in itertools.product([0,1],repeat=k):\n",
    "        out.append(i)\n",
    "    return out\n",
    "\n",
    "def flipped_choice(choice):\n",
    "    \"\"\" returns the compliment of a query\"\"\"\n",
    "    return tuple([abs(1-x) for x in choice])\n",
    "\n",
    "def gen_queries_compliment(m,n_queries):\n",
    "    \"\"\"Generates all possible queries of m binary attributes, \n",
    "    takes a sample of size n_queries from the set\"\"\"\n",
    "        \n",
    "    queries = []\n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        combs = itertools.combinations(range(m),i)\n",
    "        for comb in combs:\n",
    "            used = []\n",
    "            for choice in gen_powerset(i):\n",
    "                if choice not in used:\n",
    "                    quer = []\n",
    "                    used.append(flipped_choice(choice))\n",
    "                    quer.append(comb)\n",
    "                    quer.append(choice)\n",
    "                    queries.append(tuple(quer))\n",
    "            \n",
    "            \n",
    "    sampled = sample(queries,min(int(n_queries/2),len(queries)))\n",
    "    sample_with_compliment = list(sampled)\n",
    "    for quer in sampled:\n",
    "        sample_with_compliment.append((quer[0],flipped_choice(quer[1])))  \n",
    "    \n",
    "    return sample_with_compliment\n",
    "\n",
    "def query_seed_dict(m):\n",
    "    \"\"\" assigns a seed to every possible query based on the m attributes\"\"\"\n",
    "    quer_dict = defaultdict()\n",
    "    queries = gen_queries_uniform_complete(m,max_quers(m))\n",
    "    for query in queries:\n",
    "        quer_dict[query] = int(np.random.uniform()*100000)\n",
    "    # print(quer_dict)\n",
    "    return quer_dict\n",
    "\n",
    "def max_quers(m):\n",
    "    \"\"\"returns the maximum number of queries possible from m binary attributes\"\"\"\n",
    "    tot = 0\n",
    "    for i in range(1,m+1):\n",
    "        tot += (math.factorial(m)/(math.factorial(i)*math.factorial(m-i)))*(2**i)\n",
    "    return int(tot)\n",
    "\n",
    "def gen_query_row_pair(N_q_start,n,m,c,N_sols):\n",
    "    \"\"\" creates a database, and returns pairs of backbone solutions and the additional queries \n",
    "    (and their values) it took to find that backbone solution. Starts with N_q_start queries\n",
    "    and considers N_sols to create the backbone solutions\"\"\"\n",
    "    \n",
    "    max_quer = max_quers(m)\n",
    "    print(max_quer)\n",
    "    noise = c\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    display(db)\n",
    "    \n",
    "    # might not need to be shuffled, just making sure\n",
    "    quers = gen_queries_comp(m)\n",
    "    np.random.shuffle(quers)\n",
    "    quer_seed_dict = query_seed_dict(m)\n",
    "    quer_dict_all = get_counts_uniform_seed(db,quers,noise,quer_seed_dict)\n",
    "\n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    return_lst = []\n",
    "    backbones = []\n",
    "    fixed_sols =[]\n",
    "    quer_set = set()\n",
    "\n",
    "    N_q = min(int(N_q_start),max_quer)\n",
    "    dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n",
    "    \n",
    "    while (len(backbones) != n) and (N_q != max_quer+1):\n",
    "        \n",
    "        queries = quers[0:N_q]\n",
    "        quer_dict = dict((k, quer_dict_all[k]) for k in queries)\n",
    "        sols_all = gen_sols(n,m,quer_dict,noise,N_sols)\n",
    "        sols = np.array(sols_all[0], dtype=int) \n",
    "        \n",
    "        if len(sols) < N_sols:\n",
    "            fixed_sols = check_fixed_sols(sols)\n",
    "            \n",
    "        if len(fixed_sols) > len(backbones):\n",
    "\n",
    "            fs_c = Counter(fixed_sols)\n",
    "            b_c = Counter(backbones)\n",
    "            if sum(fs_c.values()) > sum(b_c.values()):\n",
    "                new_bones = [x for x in fs_c-b_c]\n",
    "            crit_query = {queries[N_q-1]: quer_dict[queries[N_q-1]]}\n",
    "            return_lst.append((new_bones,crit_query,dictfilt(quer_dict,set(queries).difference(quer_set))))\n",
    "            backbones += new_bones\n",
    "            quer_set.update(set(queries))\n",
    "        N_q += 1\n",
    "        \n",
    "    if N_q == max_quer -1:\n",
    "        print(\"Used all queries\")\n",
    "    return return_lst\n",
    "        \n",
    "def get_results_old(c,n,m,N_q,N_sols,dist,n_part,m_part):\n",
    "    \"\"\" Returns some performance metrics for a randomly generated database, with random noise\"\"\"   \n",
    "    noise = c\n",
    "    db = gen_bin_data_set(n,m)\n",
    "    # print(db)\n",
    "    quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "    if n_part == 0 or m_part == 0:\n",
    "        partial_dict = None\n",
    "    else:\n",
    "        partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "    \n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform(db,quers,noise)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle(db,quers,noise)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "    \n",
    "    \n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    # print(db_tup)\n",
    "\n",
    "    #solving:\n",
    "    new_n = n \n",
    "    \n",
    "    sols_all = gen_sols(new_n,m,quer_dict,partial_dict,noise,N_sols)\n",
    "    sols = np.array(sols_all[0], dtype=int) \n",
    "    time = sols_all[1] \n",
    "    \n",
    "    backbone_size = 0\n",
    "    if len(sols) < N_sols:\n",
    "        fixed_sols = check_fixed_sols(sols)\n",
    "        backbone_size = len(fixed_sols)/n\n",
    "    \n",
    "    row_sims = []\n",
    "    tot_sims = []\n",
    "    counter_sols = []\n",
    "    \n",
    "    for sol in sols:\n",
    "        sol_tup = [tuple(x) for x in sol]\n",
    "        # print(sol_tup)\n",
    "        counter_sols.append(Counter(list(sol_tup)))\n",
    "        sol_tup, common = sol_tup[:], [ e for e in db_tup if e in sol_tup and (sol_tup.pop(sol_tup.index(e)) or True)]\n",
    "        # tot_sims.append(ass_ILP(db_tup,sol_tup))\n",
    "        row_sims.append(len(common)/n)\n",
    "        \n",
    "    crit_query_count = 0\n",
    "    query_items = quer_dict.items()\n",
    "    for query in gen_crit_queries(db_tup,m,c):\n",
    "        if query in query_items:\n",
    "            crit_query_count += 1\n",
    "    \n",
    "    returns = {\n",
    "        'maximum queries': max_quers(m),\n",
    "        'backbone_size': backbone_size, \n",
    "        # 'tot_avg': np.mean(tot_sims), \n",
    "        # 'tot_min': np.min(tot_sims),\n",
    "        'row_avg': np.mean(row_sims), \n",
    "        # 'row_min': np.min(row_sims), \n",
    "        'solve_time': time, \n",
    "        'N_sols_actual': len(sols),\n",
    "        \"Contained\": Counter(db_tup) in counter_sols,\n",
    "        \"crit_queries\": crit_query_count\n",
    "    }\n",
    "    # print(counter_sols)\n",
    "    return returns\n",
    "\n",
    "def get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict):\n",
    "    \"\"\" Returns some performance metrics for a randomly generated database, with random noise....\n",
    "    \n",
    "    db: is database as a pandas array\n",
    "    seed_dict: a dictionary assigning a seed to every possible query\n",
    "    quers: a bunch of queries in a two-tuple form, will get turned into a noisy count based on 'db', 'dist' and 'seed_dict'\n",
    "    c: bound on noise, integer\n",
    "    n: size of database (eventually want this to get rid of this...)\n",
    "    m: number of attributes\n",
    "    N_sols: number of solutions to enumerate when solving\n",
    "    dist: rn either 'uniform' or 'triangle', the distribution of the noise\n",
    "    partial_dict: dictionary containing partial information.\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    \n",
    "    noise = c\n",
    "#     # print(db)\n",
    "#     quers = gen_queries_uniform_complete(m,N_q)\n",
    "    \n",
    "#     if n_part == 0 or m_part == 0:\n",
    "#         partial_dict = None\n",
    "#     else:\n",
    "#         partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "\n",
    "    if dist == 'uniform':\n",
    "        quer_dict = get_counts_uniform_seed(db,quers,noise,quer_seed_dict)\n",
    "    elif dist == 'triangle':\n",
    "        quer_dict = get_counts_triangle_seed(db,quers,noise,quer_seed_dict)\n",
    "    else:\n",
    "        return \"unrecognised distribution\"\n",
    "    \n",
    "    \n",
    "    db_tup = sorted([tuple(x) for x in list(db.to_numpy())],key=binatodeci)\n",
    "    # print(db_tup)\n",
    "\n",
    "    #solving:\n",
    "    new_n = n \n",
    "    \n",
    "    sols_all = gen_sols(new_n,m,quer_dict,partial_dict,noise,N_sols)\n",
    "    sols = np.array(sols_all[0], dtype=int) \n",
    "    time = sols_all[1] \n",
    "    \n",
    "    backbone_size = 0\n",
    "    if len(sols) < N_sols:\n",
    "        fixed_sols = check_fixed_sols(sols)\n",
    "        backbone_size = len(fixed_sols)/n\n",
    "    \n",
    "    row_sims = []\n",
    "    tot_sims = []\n",
    "    counter_sols = []\n",
    "    \n",
    "    \n",
    "    sol_set = set()\n",
    "    for sol in sols:\n",
    "        sol_tup = [tuple(x) for x in sol]\n",
    "        sol_set.add(tuple(sol_tup))\n",
    "        # print(sol_tup)\n",
    "        counter_sols.append(Counter(list(sol_tup)))\n",
    "        sol_tup, common = sol_tup[:], [ e for e in db_tup if e in sol_tup and (sol_tup.pop(sol_tup.index(e)) or True)]\n",
    "        # tot_sims.append(ass_ILP(db_tup,sol_tup))\n",
    "        row_sims.append(len(common)/n)\n",
    "        \n",
    "    crit_query_count = 0\n",
    "    query_items = quer_dict.items()\n",
    "    for query in gen_crit_queries(db_tup,m,c):\n",
    "        if query in query_items:\n",
    "            crit_query_count += 1\n",
    "            \n",
    "    \n",
    "    # print(len(sol_set))\n",
    "    # print(sols)\n",
    "    returns = {\n",
    "        'maximum queries': max_quers(m),\n",
    "        'backbone_size': backbone_size, \n",
    "        # 'tot_avg': np.mean(tot_sims), \n",
    "        # 'tot_min': np.min(tot_sims),\n",
    "        'row_avg': np.mean(row_sims), \n",
    "        # 'row_min': np.min(row_sims), \n",
    "        'solve_time': time, \n",
    "        'N_sols_actual': len(sols),\n",
    "        \"Contained\": Counter(db_tup) in counter_sols,\n",
    "        \"crit_queries\": crit_query_count\n",
    "    }\n",
    "    # print(counter_sols)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def gen_crit_queries(db,m,c):\n",
    "    outputs = []\n",
    "    A = tuple(range(m))\n",
    "    for row in db:\n",
    "        V = tuple(row)\n",
    "        for i in range(3):\n",
    "            outputs.append(((A,V),c+i))\n",
    "    return outputs\n",
    "\n",
    "def compare_noise_types(n,m,c,N_q,N_sols,N_trials,n_part,m_part):\n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    tri_list = []\n",
    "    uni_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    quer_seed_dict = None\n",
    "    \n",
    "    for i in range(N_trials):\n",
    "        for dist in ['triangle','uniform']:\n",
    "            quers = gen_queries_uniform_complete(m,N_q)\n",
    "            db = gen_bin_data_set(n,m)\n",
    "            partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "            \n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist)\n",
    "            results[dist].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[dist] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for dist in ['triangle','uniform']:\n",
    "        df = pd.DataFrame(results[dist])\n",
    "        out_results[dist] = dict(df.mean()) \n",
    "        out_results[dist]['frac_solved_uniqely'] = unique_counts[dist]/N_trials\n",
    "        out_results[dist]['params'] = {'n': n, 'm': m, 'c': c, 'N_q': N_q, 'N_sols': N_sols, 'N_trials': N_trials}\n",
    "    return out_results\n",
    "\n",
    "# def compare_query_types(n,m,c,N_q,N_sols,N_trials):\n",
    "#     \"\"\" don't think this actually does anything different to compare noise types\n",
    "#     \"\"\"\n",
    "#     # results = defaultdict(lambda: defaultdict())\n",
    "#     results = defaultdict(list)\n",
    "#     tri_list = []\n",
    "#     uni_list = []\n",
    "#     unique_counts = defaultdict(lambda:0)\n",
    "#     for i in range(N_trials):\n",
    "#         for dist in ['triangle','uniform']:\n",
    "#             temp_res = get_results(c,n,m,N_q,N_sols,dist)\n",
    "#             results[dist].append(temp_res)\n",
    "#             if temp_res['N_sols_actual'] == 1:\n",
    "#                 unique_counts[dist] += 1\n",
    "    \n",
    "#     out_results = defaultdict(lambda: defaultdict())\n",
    "#     for dist in ['triangle','uniform']:\n",
    "#         df = pd.DataFrame(results[dist])\n",
    "#         out_results[dist] = dict(df.mean()) \n",
    "#         out_results[dist]['frac_solved_uniqely'] = unique_counts[dist]/N_trials\n",
    "#         # out_results[]\n",
    "#     return out_results\n",
    "\n",
    "def compare_partial_info(n,m,c,N_q,N_sols,N_trials,dist,n_part,m_part):\n",
    "    \"\"\"need to fix this so it's using seed dicts\"\"\"\n",
    "    \n",
    "    # results = defaultdict(lambda: defaultdict())\n",
    "    results = defaultdict(list)\n",
    "    normal_list = []\n",
    "    partial_list = []\n",
    "    unique_counts = defaultdict(lambda:0)\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        for partial in [[0,0],[n_part,m_part]]:\n",
    "            partial_dict = gen_partial_info(db,partial[0],partial[1])\n",
    "            # print(partial_dict)\n",
    "            temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "            # temp_res = get_results(c,n,m,N_q,N_sols,dist,partial[0],partial[1])\n",
    "            results[str(partial)].append(temp_res)\n",
    "            if temp_res['N_sols_actual'] == 1:\n",
    "                unique_counts[str(partial)] += 1\n",
    "    \n",
    "    out_results = defaultdict(lambda: defaultdict())\n",
    "    for partial in [str([0,0]),str([n_part,m_part])]:\n",
    "        df = pd.DataFrame(results[partial])\n",
    "        out_results[partial] = dict(df.mean()) \n",
    "        out_results[partial]['frac_solved_uniqely'] = unique_counts[partial]/N_trials\n",
    "        # out_results[]\n",
    "    return out_results\n",
    "\n",
    "def single_basic_run(n,m,c,N_q,N_sols,N_trials,dist,n_part,m_part):\n",
    "    \"\"\" Generates a dataset and solves, averaging over N_trials times.\n",
    "    returns a dictionary with all the performance 'metrics'\"\"\"\n",
    "    \n",
    "    out_lst = []\n",
    "    unique_counts = 0\n",
    "    for i in range(N_trials):\n",
    "        quer_seed_dict = query_seed_dict(m)\n",
    "        quers = gen_queries_uniform_complete(m,N_q)\n",
    "        db = gen_bin_data_set(n,m)\n",
    "        # print(db)\n",
    "        partial_dict = gen_partial_info(db,n_part,m_part)\n",
    "        # print(partial_dict)\n",
    "        temp_res = get_results(db,quer_seed_dict,quers,c,n,m,N_sols,dist,partial_dict)\n",
    "        out_lst.append(temp_res)\n",
    "        if temp_res['N_sols_actual'] == 1:\n",
    "            unique_counts += 1\n",
    "            \n",
    "    df = pd.DataFrame(out_lst)\n",
    "    out_dict = dict(df.mean()) \n",
    "    out_dict['frac_solved_uniqely'] = unique_counts/N_trials\n",
    "\n",
    "    return out_dict\n",
    "    \n",
    "    \n",
    "        \n",
    "# need to write a function which compares ordered sets of queries over a range of number of queries\n",
    "# for a choice of n and m, averaging performance over n_trials?\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ee0c336-e7e0-4625-ba92-2b529b01420e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.compare_noise_types.<locals>.<lambda>()>,\n",
       "            {'triangle': {'maximum queries': 728.0,\n",
       "              'backbone_size': 1.0,\n",
       "              'row_avg': 1.0,\n",
       "              'solve_time': 0.2295980453491211,\n",
       "              'N_sols_actual': 1.0,\n",
       "              'Contained': 1.0,\n",
       "              'crit_queries': 5.0,\n",
       "              'frac_solved_uniqely': 1.0,\n",
       "              'params': {'n': 5,\n",
       "               'm': 6,\n",
       "               'c': 1,\n",
       "               'N_q': 728,\n",
       "               'N_sols': 10000,\n",
       "               'N_trials': 1}},\n",
       "             'uniform': {'maximum queries': 728.0,\n",
       "              'backbone_size': 1.0,\n",
       "              'row_avg': 1.0,\n",
       "              'solve_time': 0.17855501174926758,\n",
       "              'N_sols_actual': 1.0,\n",
       "              'Contained': 1.0,\n",
       "              'crit_queries': 2.0,\n",
       "              'frac_solved_uniqely': 1.0,\n",
       "              'params': {'n': 5,\n",
       "               'm': 6,\n",
       "               'c': 1,\n",
       "               'N_q': 728,\n",
       "               'N_sols': 10000,\n",
       "               'N_trials': 1}}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 1\n",
    "n = 5\n",
    "m = 6\n",
    "N_q = 728\n",
    "N_sols = 10000\n",
    "N_trials = 1\n",
    "compare_noise_types(n,m,c,N_q,N_sols,N_trials,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70401c61-0a8c-40bc-b6d9-a11f170e488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2\n",
    "n = 3\n",
    "m = 10\n",
    "N_q = 5330\n",
    "N_sols = 3\n",
    "n_part = 2\n",
    "m_part = 7\n",
    "\n",
    "display(get_results(c,n,m,N_q,N_sols,'uniform',n_part,m_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf41bb9c-5093-44f7-9c70-0e350dd1ec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'maximum queries': 242.0,\n",
       " 'backbone_size': 1.0,\n",
       " 'row_avg': 1.0,\n",
       " 'solve_time': 0.03753342628479004,\n",
       " 'N_sols_actual': 1.0,\n",
       " 'Contained': 1.0,\n",
       " 'crit_queries': 2.64,\n",
       " 'frac_solved_uniqely': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = 2\n",
    "n = 10\n",
    "m = 5\n",
    "N_q = 142\n",
    "N_sols = 1000\n",
    "N_trials = 50\n",
    "n_part = 10\n",
    "m_part = 5\n",
    "dist = 'uniform'\n",
    "\n",
    "display(single_basic_run(n,m,c,N_q,N_sols,N_trials,dist,n_part,m_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2729d9c5-a72e-4b11-9fa6-6fcf2fa483ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2\n",
    "n = 10\n",
    "m = 5\n",
    "# N_q = 150\n",
    "N_sols = 1000\n",
    "N_trials = 3\n",
    "n_part = 10\n",
    "m_part = 5\n",
    "dist = 'uniform'\n",
    "\n",
    "data = defaultdict()\n",
    "for N_q in np.linspace(100,150,2,dtype = int):\n",
    "    data[int(N_q)] = compare_partial_info(n,m,c,N_q,N_sols,N_trials,dist,n_part,m_part)\n",
    "\n",
    "filename = 'mm/poo.json'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'w') as f: \n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2aaca5-94ba-4c24-8d57-392d96db9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_0</th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   att_0  att_1  att_2\n",
       "0      0      1      1\n",
       "1      1      0      0\n",
       "2      0      0      1\n",
       "3      1      0      1\n",
       "4      0      0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[([(0, 0, 1)],\n",
       "  {((0, 2), (1, 1)): 1},\n",
       "  {((0, 2), (0, 1)): 2,\n",
       "   ((0, 2), (0, 0)): 0,\n",
       "   ((0, 1, 2), (1, 1, 1)): 0,\n",
       "   ((2,), (1,)): 5,\n",
       "   ((0, 1, 2), (0, 0, 0)): 1,\n",
       "   ((1, 2), (0, 1)): 2,\n",
       "   ((0,), (0,)): 3,\n",
       "   ((1,), (0,)): 5,\n",
       "   ((0, 2), (1, 1)): 1}),\n",
       " ([(0, 0, 1), (1, 0, 0)],\n",
       "  {((0, 1, 2), (1, 0, 0)): 2},\n",
       "  {((0, 1, 2), (0, 1, 1)): 1,\n",
       "   ((0, 1), (0, 0)): 2,\n",
       "   ((0, 1), (1, 0)): 1,\n",
       "   ((0,), (1,)): 3,\n",
       "   ((1,), (1,)): 2,\n",
       "   ((0, 1, 2), (1, 0, 0)): 2}),\n",
       " ([(0, 1, 1), (1, 0, 1)],\n",
       "  {((0, 1), (0, 1)): 2},\n",
       "  {((1, 2), (0, 0)): 0,\n",
       "   ((0, 1, 2), (1, 0, 1)): 1,\n",
       "   ((2,), (0,)): 0,\n",
       "   ((0, 1), (1, 1)): 0,\n",
       "   ((0, 1, 2), (0, 0, 1)): 2,\n",
       "   ((0, 1, 2), (1, 1, 0)): 1,\n",
       "   ((1, 2), (1, 0)): 0,\n",
       "   ((1, 2), (1, 1)): 0,\n",
       "   ((0, 2), (1, 0)): 1,\n",
       "   ((0, 1, 2), (0, 1, 0)): 0,\n",
       "   ((0, 1), (0, 1)): 2})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_q_start = 1\n",
    "n=5\n",
    "m=3\n",
    "c=1\n",
    "N_sols = 10000\n",
    "display(gen_query_row_pair(N_q_start,n,m,c,N_sols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "116df9b5-b6cc-41a0-83ae-127f753166fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'b': 2}\n",
    "aa = {'b': 2, 'c':3}\n",
    "('b', 2) in aa.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee6469bb-7f2e-4bb6-8805-3654a9f8db10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tuple(range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d018ec-1d81-46ce-b122-8fa106557abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "def add1():\n",
    "    y_dict = defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ad78ce-12a8-449b-ae80-8036612924a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_dict11 = defaultdict(lambda:0)\n",
    "for i in partial_dict11.keys():\n",
    "    print('yo')\n",
    "    print(partial_dict11[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9512b3e4-255e-43f5-8493-0ca3c10db1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_seed_dict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f55b33-d7a8-4afe-8328-3752a46759b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
